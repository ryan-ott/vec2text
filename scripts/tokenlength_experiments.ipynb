{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from evaluate import load # For SacreBLEU\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following code runs the plots for different metrics on 3 datasets vs token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\n",
      "Output directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\n",
      "Plot saved to: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/bleu_score_vs_max_token_all_datasets.png\n",
      "Plot saved to: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/f1_score_vs_max_token_all_datasets.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def extract_metrics(log_file):\n",
    "    \"\"\"\n",
    "    Extracts BLEU scores, F1 scores, and max token length from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing max token length, BLEU score, F1 score, and dataset name, \n",
    "        or None if not found.\n",
    "    \"\"\"\n",
    "    max_token = None\n",
    "    bleu_score = None\n",
    "    f1_score = None\n",
    "    dataset_name = None\n",
    "\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"eval_bleu_score:\" in line:\n",
    "                bleu_score = float(line.split(\"eval_bleu_score:\")[1].strip())\n",
    "            elif \"eval_token_set_f1:\" in line:\n",
    "                f1_score = float(line.split(\"eval_token_set_f1:\")[1].strip())\n",
    "            elif \"maxtoken\" in log_file:\n",
    "                max_token = int(log_file.split(\"_\")[-1].replace(\"maxtoken.log\", \"\"))\n",
    "\n",
    "        # Extract dataset name from the filename\n",
    "        match = re.search(r\"_(.*?)_500samples_\", log_file)\n",
    "        if match:\n",
    "            dataset_name = match.group(1)\n",
    "\n",
    "    if max_token is not None and bleu_score is not None and f1_score is not None and dataset_name is not None:\n",
    "        return max_token, bleu_score, f1_score, dataset_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_metrics_vs_max_token(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Plots BLEU scores and F1 scores against max token lengths for multiple datasets in a single plot.\n",
    "\n",
    "    Args:\n",
    "        all_data: A dictionary where keys are dataset names and values are dictionaries \n",
    "                  of max token lengths to lists of (BLEU score, F1 score) tuples.\n",
    "        output_dir: The directory to save the plot to.\n",
    "    \"\"\"\n",
    "    for metric_name, metric_index in zip([\"BLEU Score\", \"F1 Score\"], [0,1]):\n",
    "      plt.figure(figsize=(10, 6))  # Adjust figure size for better readability\n",
    "  \n",
    "      for dataset_name, data in all_data.items():\n",
    "          max_tokens = sorted(data.keys())\n",
    "          metric_scores = [data[max_token] for max_token in max_tokens]\n",
    "  \n",
    "          means = [np.mean([scores[metric_index] for scores in met_scores]) for met_scores in metric_scores]\n",
    "          stds = [np.std([scores[metric_index] for scores in met_scores]) for met_scores in metric_scores]\n",
    "  \n",
    "          plt.errorbar(max_tokens, means, yerr=stds, fmt='o-', capsize=5, label=dataset_name)\n",
    "  \n",
    "      plt.xlabel(\"Max Token Length\")\n",
    "      plt.ylabel(metric_name)\n",
    "      plt.title(f\"{metric_name} vs. Max Token Length (Multiple Datasets)\")\n",
    "      plt.grid(True)\n",
    "      plt.legend()  # Show the legend\n",
    "  \n",
    "      # Save the plot\n",
    "      output_filename = f\"{metric_name.lower().replace(' ', '_')}_vs_max_token_all_datasets.png\"\n",
    "      output_path = os.path.join(output_dir, output_filename)\n",
    "      plt.savefig(output_path)\n",
    "      print(f\"Plot saved to: {output_path}\")\n",
    "      plt.close()\n",
    "\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    log_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\"\n",
    "    output_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\"\n",
    "    # ---------------------\n",
    "\n",
    "    print(f\"Log directory: {log_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    all_data = {}  # Dictionary to store data for all datasets\n",
    "\n",
    "    for filename in os.listdir(log_dir):\n",
    "        if filename.endswith(\".log\"):\n",
    "            filepath = os.path.join(log_dir, filename)\n",
    "            result = extract_metrics(filepath)\n",
    "\n",
    "            if result:\n",
    "                max_token, bleu_score, f1_score, dataset_name = result\n",
    "                if dataset_name not in all_data:\n",
    "                    all_data[dataset_name] = {}\n",
    "                if max_token not in all_data[dataset_name]:\n",
    "                    all_data[dataset_name][max_token] = []\n",
    "                all_data[dataset_name][max_token].append((bleu_score, f1_score))\n",
    "\n",
    "    if all_data:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plot_metrics_vs_max_token(all_data, output_dir)\n",
    "    else:\n",
    "        print(\"No valid data found in log files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\n",
      "Output directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_token_f1(pred, label):\n",
    "    \"\"\"\n",
    "    Calculates the token-level F1 score between two strings.\n",
    "\n",
    "    Args:\n",
    "        pred: The predicted string.\n",
    "        label: The true label string.\n",
    "\n",
    "    Returns:\n",
    "        The token-level F1 score.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(pred.lower().split())\n",
    "    label_tokens = set(label.lower().split())\n",
    "\n",
    "    if not pred_tokens and not label_tokens:\n",
    "        return 1.0  # Both empty\n",
    "\n",
    "    if not pred_tokens or not label_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    precision = len(pred_tokens.intersection(label_tokens)) / len(pred_tokens)\n",
    "    recall = len(pred_tokens.intersection(label_tokens)) / len(label_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def extract_metrics(log_file_path):\n",
    "    \"\"\"\n",
    "    Extracts exact match, token F1, SBERT score, BLEU, and max token length from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing lists of exact match, token F1, SBERT score,\n",
    "        BLEU, max token length, and dataset name, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_line(line):\n",
    "        \"\"\"Removes 'Index X:' prefix and extra spacing from a line.\"\"\"\n",
    "        line = re.sub(r\"^Index \\d+: \", \"\", line)\n",
    "        line = line.strip()\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        line = re.sub(r\"\\s*/\\s*\", \"/\", line)\n",
    "        return line\n",
    "\n",
    "    preds = {}\n",
    "    labels = {}\n",
    "\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    preds_section = False\n",
    "    labels_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Contents of decoded_preds:\" in line:\n",
    "            preds_section = True\n",
    "            labels_section = False\n",
    "            continue\n",
    "        elif \"Contents of decoded_labels:\" in line:\n",
    "            preds_section = False\n",
    "            labels_section = True\n",
    "            continue\n",
    "\n",
    "        if preds_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                preds[index] = match.group(2)\n",
    "        elif labels_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                labels[index] = match.group(2)\n",
    "\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Warning: Number of predictions ({len(preds)}) and labels ({len(labels)}) differ in {log_file_path}.\")\n",
    "\n",
    "    min_len = min(len(preds), len(labels))\n",
    "\n",
    "    # Initialize Sentence-BERT model and SacreBLEU\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    bleu = load(\"sacrebleu\")\n",
    "\n",
    "    exact_matches = []\n",
    "    token_f1s = []\n",
    "    sbert_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    data_added_to_bleu = False  # Flag to track if data was added to bleu\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if i in preds and i in labels:\n",
    "            cleaned_pred = clean_line(preds[i])\n",
    "            cleaned_label = clean_line(labels[i])\n",
    "\n",
    "            # Exact Match\n",
    "            exact_match = 1.0 if cleaned_pred == cleaned_label else 0.0\n",
    "            exact_matches.append(exact_match)\n",
    "\n",
    "            # Token F1\n",
    "            token_f1 = calculate_token_f1(cleaned_pred, cleaned_label)\n",
    "            token_f1s.append(token_f1)\n",
    "\n",
    "            # Sentence-BERT\n",
    "            pred_embedding = model.encode(cleaned_pred, convert_to_tensor=True)\n",
    "            label_embedding = model.encode(cleaned_label, convert_to_tensor=True)\n",
    "            cosine_sim = util.pytorch_cos_sim(pred_embedding, label_embedding)\n",
    "            sbert_scores.append(cosine_sim.item())\n",
    "\n",
    "            # SacreBLEU\n",
    "            bleu.add_batch(predictions=[cleaned_pred], references=[[cleaned_label]])\n",
    "            data_added_to_bleu = True\n",
    "\n",
    "    # Compute BLEU only if data was added\n",
    "    if data_added_to_bleu:\n",
    "        bleu_results = bleu.compute()\n",
    "        bleu_scores = [bleu_results[\"score\"]] * min_len\n",
    "    else:\n",
    "        bleu_scores = [0.0] * min_len\n",
    "\n",
    "    # Extract max token and dataset name from filename\n",
    "    if \"maxtoken\" in log_file_path:\n",
    "        max_token = int(log_file_path.split(\"_\")[-1].replace(\"maxtoken.log\", \"\"))\n",
    "    else:\n",
    "        max_token = None\n",
    "\n",
    "    match = re.search(r\"_(.*?)_500samples_\", log_file_path)\n",
    "    dataset_name = match.group(1) if match else None\n",
    "\n",
    "    if max_token is not None and dataset_name is not None:\n",
    "        return {\n",
    "            \"max_token\": max_token,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"token_f1\": token_f1s,\n",
    "            \"sbert_score\": sbert_scores,  # Use sbert_score\n",
    "            \"bleu\": bleu_scores,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_metrics_vs_max_token(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Plots exact match, token F1, SBERT score, and BLEU against max token lengths for multiple datasets on the same plot.\n",
    "\n",
    "    Args:\n",
    "        all_data: A dictionary where keys are dataset names and values are dictionaries\n",
    "                  of max token lengths to lists of metrics.\n",
    "        output_dir: The directory to save the plots to.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": \"Exact Match\",\n",
    "        \"token_f1\": \"Token F1\",\n",
    "        \"sbert_score\": \"SBERT Score\",\n",
    "        \"bleu\": \"BLEU\"\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_label in metrics.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for dataset_name, data in all_data.items():\n",
    "            max_tokens = sorted(data.keys())\n",
    "            means = [np.mean(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "            stds = [np.std(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "\n",
    "            plt.errorbar(max_tokens, means, yerr=stds, fmt='o-', capsize=5, label=dataset_name)\n",
    "\n",
    "        plt.xlabel(\"Max Token Length\")\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.title(f\"{metric_label} vs. Max Token Length\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        output_path = os.path.join(output_dir, f\"{metric_name}_vs_max_token_all_datasets.png\")\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"{metric_label} plot saved to: {output_path}\")\n",
    "        plt.close()\n",
    "\n",
    "# --- Configuration ---\n",
    "log_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\"\n",
    "output_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "all_data = {}  # Dictionary to store data for all datasets\n",
    "\n",
    "for filename in os.listdir(log_dir):\n",
    "    if filename.endswith(\".log\"):\n",
    "        filepath = os.path.join(log_dir, filename)\n",
    "        result = extract_metrics(filepath)\n",
    "\n",
    "        if result:\n",
    "            max_token = result[\"max_token\"]\n",
    "            dataset_name = result[\"dataset_name\"]\n",
    "            if dataset_name not in all_data:\n",
    "                all_data[dataset_name] = {}\n",
    "            if max_token not in all_data[dataset_name]:\n",
    "                all_data[dataset_name][max_token] = {\n",
    "                    \"exact_match\": [],\n",
    "                    \"token_f1\": [],\n",
    "                    \"sbert_score\": [],  # Use sbert_score here\n",
    "                    \"bleu\": []\n",
    "                }\n",
    "            all_data[dataset_name][max_token][\"exact_match\"].extend(result[\"exact_match\"])\n",
    "            all_data[dataset_name][max_token][\"token_f1\"].extend(result[\"token_f1\"])\n",
    "            all_data[dataset_name][max_token][\"sbert_score\"].extend(result[\"sbert_score\"])  # Extend sbert_score\n",
    "            all_data[dataset_name][max_token][\"bleu\"].extend(result[\"bleu\"])\n",
    "\n",
    "if all_data:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_metrics_vs_max_token(all_data, output_dir)\n",
    "else:\n",
    "    print(\"No valid data found in log files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\n",
      "Output directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_token_f1(pred, label):\n",
    "    \"\"\"\n",
    "    Calculates the token-level F1 score between two strings.\n",
    "\n",
    "    Args:\n",
    "        pred: The predicted string.\n",
    "        label: The true label string.\n",
    "\n",
    "    Returns:\n",
    "        The token-level F1 score.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(pred.lower().split())\n",
    "    label_tokens = set(label.lower().split())\n",
    "\n",
    "    if not pred_tokens and not label_tokens:\n",
    "        return 1.0  # Both empty\n",
    "\n",
    "    if not pred_tokens or not label_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    precision = len(pred_tokens.intersection(label_tokens)) / len(pred_tokens)\n",
    "    recall = len(pred_tokens.intersection(label_tokens)) / len(label_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def extract_metrics(log_file_path):\n",
    "    \"\"\"\n",
    "    Extracts exact match, token F1, SBERT score, BLEU, and max token length from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing lists of exact match, token F1, SBERT score,\n",
    "        BLEU, max token length, and dataset name, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_line(line):\n",
    "        \"\"\"Removes 'Index X:' prefix and extra spacing from a line.\"\"\"\n",
    "        line = re.sub(r\"^Index \\d+: \", \"\", line)\n",
    "        line = line.strip()\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        line = re.sub(r\"\\s*/\\s*\", \"/\", line)\n",
    "        return line\n",
    "\n",
    "    preds = {}\n",
    "    labels = {}\n",
    "\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    preds_section = False\n",
    "    labels_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Contents of decoded_preds:\" in line:\n",
    "            preds_section = True\n",
    "            labels_section = False\n",
    "            continue\n",
    "        elif \"Contents of decoded_labels:\" in line:\n",
    "            preds_section = False\n",
    "            labels_section = True\n",
    "            continue\n",
    "\n",
    "        if preds_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                preds[index] = match.group(2)\n",
    "        elif labels_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                labels[index] = match.group(2)\n",
    "\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Warning: Number of predictions ({len(preds)}) and labels ({len(labels)}) differ in {log_file_path}.\")\n",
    "\n",
    "    min_len = min(len(preds), len(labels))\n",
    "\n",
    "    # Initialize Sentence-BERT model and SacreBLEU\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    bleu = load(\"sacrebleu\")\n",
    "\n",
    "    exact_matches = []\n",
    "    token_f1s = []\n",
    "    sbert_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    data_added_to_bleu = False  # Flag to track if data was added to bleu\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if i in preds and i in labels:\n",
    "            cleaned_pred = clean_line(preds[i])\n",
    "            cleaned_label = clean_line(labels[i])\n",
    "\n",
    "            # Exact Match\n",
    "            exact_match = 1.0 if cleaned_pred == cleaned_label else 0.0\n",
    "            exact_matches.append(exact_match)\n",
    "\n",
    "            # Token F1\n",
    "            token_f1 = calculate_token_f1(cleaned_pred, cleaned_label)\n",
    "            token_f1s.append(token_f1)\n",
    "\n",
    "            # Sentence-BERT\n",
    "            pred_embedding = model.encode(cleaned_pred, convert_to_tensor=True)\n",
    "            label_embedding = model.encode(cleaned_label, convert_to_tensor=True)\n",
    "            cosine_sim = util.pytorch_cos_sim(pred_embedding, label_embedding)\n",
    "            sbert_scores.append(cosine_sim.item())\n",
    "\n",
    "            # SacreBLEU\n",
    "            bleu.add_batch(predictions=[cleaned_pred], references=[[cleaned_label]])\n",
    "            data_added_to_bleu = True\n",
    "\n",
    "    # Compute BLEU only if data was added\n",
    "    if data_added_to_bleu:\n",
    "        bleu_results = bleu.compute()\n",
    "        bleu_scores = [bleu_results[\"score\"]] * min_len\n",
    "    else:\n",
    "        bleu_scores = [0.0] * min_len\n",
    "\n",
    "    # Extract max token and dataset name from filename\n",
    "    if \"maxtoken\" in log_file_path:\n",
    "        max_token = int(log_file_path.split(\"_\")[-1].replace(\"maxtoken.log\", \"\"))\n",
    "    else:\n",
    "        max_token = None\n",
    "\n",
    "    match = re.search(r\"_(.*?)_500samples_\", log_file_path)\n",
    "    dataset_name = match.group(1) if match else None\n",
    "\n",
    "    if max_token is not None and dataset_name is not None:\n",
    "        return {\n",
    "            \"max_token\": max_token,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"token_f1\": token_f1s,\n",
    "            \"sbert_score\": sbert_scores,  # Use sbert_score\n",
    "            \"bleu\": bleu_scores,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_metrics_vs_max_token(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Plots exact match, token F1, SBERT score, and BLEU against max token lengths for multiple datasets on the same plot.\n",
    "\n",
    "    Args:\n",
    "        all_data: A dictionary where keys are dataset names and values are dictionaries\n",
    "                  of max token lengths to lists of metrics.\n",
    "        output_dir: The directory to save the plots to.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": \"Exact Match\",\n",
    "        \"token_f1\": \"Token F1\",\n",
    "        \"sbert_score\": \"SBERT Score\",\n",
    "        \"bleu\": \"BLEU\"\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_label in metrics.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for dataset_name, data in all_data.items():\n",
    "            max_tokens = sorted(data.keys())\n",
    "            means = [np.mean(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "            stds = [np.std(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "\n",
    "            plt.errorbar(max_tokens, means, yerr=stds, fmt='o-', capsize=5, label=dataset_name)\n",
    "\n",
    "        plt.xlabel(\"Max Token Length\")\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.title(f\"{metric_label} vs. Max Token Length\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        output_path = os.path.join(output_dir, f\"{metric_name}_vs_max_token_all_datasets.png\")\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"{metric_label} plot saved to: {output_path}\")\n",
    "        plt.close()\n",
    "\n",
    "# --- Configuration ---\n",
    "log_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\"\n",
    "output_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "all_data = {}  # Dictionary to store data for all datasets\n",
    "\n",
    "for filename in os.listdir(log_dir):\n",
    "    if filename.endswith(\".log\"):\n",
    "        filepath = os.path.join(log_dir, filename)\n",
    "        result = extract_metrics(filepath)\n",
    "\n",
    "        if result:\n",
    "            max_token = result[\"max_token\"]\n",
    "            dataset_name = result[\"dataset_name\"]\n",
    "            if dataset_name not in all_data:\n",
    "                all_data[dataset_name] = {}\n",
    "            if max_token not in all_data[dataset_name]:\n",
    "                all_data[dataset_name][max_token] = {\n",
    "                    \"exact_match\": [],\n",
    "                    \"token_f1\": [],\n",
    "                    \"sbert_score\": [],  # Use sbert_score here\n",
    "                    \"bleu\": []\n",
    "                }\n",
    "            all_data[dataset_name][max_token][\"exact_match\"].extend(result[\"exact_match\"])\n",
    "            all_data[dataset_name][max_token][\"token_f1\"].extend(result[\"token_f1\"])\n",
    "            all_data[dataset_name][max_token][\"sbert_score\"].extend(result[\"sbert_score\"])  # Extend sbert_score\n",
    "            all_data[dataset_name][max_token][\"bleu\"].extend(result[\"bleu\"])\n",
    "\n",
    "if all_data:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_metrics_vs_max_token(all_data, output_dir)\n",
    "else:\n",
    "    print(\"No valid data found in log files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\n",
      "Output directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_token_f1(pred, label):\n",
    "    \"\"\"\n",
    "    Calculates the token-level F1 score between two strings.\n",
    "\n",
    "    Args:\n",
    "        pred: The predicted string.\n",
    "        label: The true label string.\n",
    "\n",
    "    Returns:\n",
    "        The token-level F1 score.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(pred.lower().split())\n",
    "    label_tokens = set(label.lower().split())\n",
    "\n",
    "    if not pred_tokens and not label_tokens:\n",
    "        return 1.0  # Both empty\n",
    "\n",
    "    if not pred_tokens or not label_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    precision = len(pred_tokens.intersection(label_tokens)) / len(pred_tokens)\n",
    "    recall = len(pred_tokens.intersection(label_tokens)) / len(label_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def extract_metrics(log_file_path):\n",
    "    \"\"\"\n",
    "    Extracts exact match, token F1, SBERT score, BLEU, and max token length from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing lists of exact match, token F1, SBERT score,\n",
    "        BLEU, max token length, and dataset name, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_line(line):\n",
    "        \"\"\"Removes 'Index X:' prefix and extra spacing from a line.\"\"\"\n",
    "        line = re.sub(r\"^Index \\d+: \", \"\", line)\n",
    "        line = line.strip()\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        line = re.sub(r\"\\s*/\\s*\", \"/\", line)\n",
    "        return line\n",
    "\n",
    "    preds = {}\n",
    "    labels = {}\n",
    "\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    preds_section = False\n",
    "    labels_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Contents of decoded_preds:\" in line:\n",
    "            preds_section = True\n",
    "            labels_section = False\n",
    "            continue\n",
    "        elif \"Contents of decoded_labels:\" in line:\n",
    "            preds_section = False\n",
    "            labels_section = True\n",
    "            continue\n",
    "\n",
    "        if preds_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                preds[index] = match.group(2)\n",
    "        elif labels_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                labels[index] = match.group(2)\n",
    "\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Warning: Number of predictions ({len(preds)}) and labels ({len(labels)}) differ in {log_file_path}.\")\n",
    "\n",
    "    min_len = min(len(preds), len(labels))\n",
    "\n",
    "    # Initialize Sentence-BERT model and SacreBLEU\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    bleu = load(\"sacrebleu\")\n",
    "\n",
    "    exact_matches = []\n",
    "    token_f1s = []\n",
    "    sbert_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    data_added_to_bleu = False  # Flag to track if data was added to bleu\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if i in preds and i in labels:\n",
    "            cleaned_pred = clean_line(preds[i])\n",
    "            cleaned_label = clean_line(labels[i])\n",
    "\n",
    "            # Exact Match\n",
    "            exact_match = 1.0 if cleaned_pred == cleaned_label else 0.0\n",
    "            exact_matches.append(exact_match)\n",
    "\n",
    "            # Token F1\n",
    "            token_f1 = calculate_token_f1(cleaned_pred, cleaned_label)\n",
    "            token_f1s.append(token_f1)\n",
    "\n",
    "            # Sentence-BERT\n",
    "            pred_embedding = model.encode(cleaned_pred, convert_to_tensor=True)\n",
    "            label_embedding = model.encode(cleaned_label, convert_to_tensor=True)\n",
    "            cosine_sim = util.pytorch_cos_sim(pred_embedding, label_embedding)\n",
    "            sbert_scores.append(cosine_sim.item())\n",
    "\n",
    "            # SacreBLEU\n",
    "            bleu.add_batch(predictions=[cleaned_pred], references=[[cleaned_label]])\n",
    "            data_added_to_bleu = True\n",
    "\n",
    "    # Compute BLEU only if data was added\n",
    "    if data_added_to_bleu:\n",
    "        bleu_results = bleu.compute()\n",
    "        bleu_scores = [bleu_results[\"score\"]] * min_len\n",
    "    else:\n",
    "        bleu_scores = [0.0] * min_len\n",
    "\n",
    "    # Extract max token and dataset name from filename\n",
    "    if \"maxtoken\" in log_file_path:\n",
    "        max_token = int(log_file_path.split(\"_\")[-1].replace(\"maxtoken.log\", \"\"))\n",
    "    else:\n",
    "        max_token = None\n",
    "\n",
    "    match = re.search(r\"_(.*?)_500samples_\", log_file_path)\n",
    "    dataset_name = match.group(1) if match else None\n",
    "\n",
    "    if max_token is not None and dataset_name is not None:\n",
    "        return {\n",
    "            \"max_token\": max_token,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"token_f1\": token_f1s,\n",
    "            \"sbert_score\": sbert_scores,  # Use sbert_score\n",
    "            \"bleu\": bleu_scores,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_metrics_vs_max_token(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Plots exact match, token F1, SBERT score, and BLEU against max token lengths for multiple datasets on the same plot.\n",
    "\n",
    "    Args:\n",
    "        all_data: A dictionary where keys are dataset names and values are dictionaries\n",
    "                  of max token lengths to lists of metrics.\n",
    "        output_dir: The directory to save the plots to.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": \"Exact Match\",\n",
    "        \"token_f1\": \"Token F1\",\n",
    "        \"sbert_score\": \"SBERT Score\",\n",
    "        \"bleu\": \"BLEU\"\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_label in metrics.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for dataset_name, data in all_data.items():\n",
    "            max_tokens = sorted(data.keys())\n",
    "            means = [np.mean(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "            stds = [np.std(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "\n",
    "            plt.errorbar(max_tokens, means, yerr=stds, fmt='o-', capsize=5, label=dataset_name)\n",
    "\n",
    "        plt.xlabel(\"Max Token Length\")\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.title(f\"{metric_label} vs. Max Token Length\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        output_path = os.path.join(output_dir, f\"{metric_name}_vs_max_token_all_datasets.png\")\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"{metric_label} plot saved to: {output_path}\")\n",
    "        plt.close()\n",
    "\n",
    "# --- Configuration ---\n",
    "log_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\"\n",
    "output_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "all_data = {}  # Dictionary to store data for all datasets\n",
    "\n",
    "for filename in os.listdir(log_dir):\n",
    "    if filename.endswith(\".log\"):\n",
    "        filepath = os.path.join(log_dir, filename)\n",
    "        result = extract_metrics(filepath)\n",
    "\n",
    "        if result:\n",
    "            max_token = result[\"max_token\"]\n",
    "            dataset_name = result[\"dataset_name\"]\n",
    "            if dataset_name not in all_data:\n",
    "                all_data[dataset_name] = {}\n",
    "            if max_token not in all_data[dataset_name]:\n",
    "                all_data[dataset_name][max_token] = {\n",
    "                    \"exact_match\": [],\n",
    "                    \"token_f1\": [],\n",
    "                    \"sbert_score\": [],  # Use sbert_score here\n",
    "                    \"bleu\": []\n",
    "                }\n",
    "            all_data[dataset_name][max_token][\"exact_match\"].extend(result[\"exact_match\"])\n",
    "            all_data[dataset_name][max_token][\"token_f1\"].extend(result[\"token_f1\"])\n",
    "            all_data[dataset_name][max_token][\"sbert_score\"].extend(result[\"sbert_score\"])  # Extend sbert_score\n",
    "            all_data[dataset_name][max_token][\"bleu\"].extend(result[\"bleu\"])\n",
    "\n",
    "if all_data:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_metrics_vs_max_token(all_data, output_dir)\n",
    "else:\n",
    "    print(\"No valid data found in log files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\n",
      "Output directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_token_f1(pred, label):\n",
    "    \"\"\"\n",
    "    Calculates the token-level F1 score between two strings.\n",
    "\n",
    "    Args:\n",
    "        pred: The predicted string.\n",
    "        label: The true label string.\n",
    "\n",
    "    Returns:\n",
    "        The token-level F1 score.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(pred.lower().split())\n",
    "    label_tokens = set(label.lower().split())\n",
    "\n",
    "    if not pred_tokens and not label_tokens:\n",
    "        return 1.0  # Both empty\n",
    "\n",
    "    if not pred_tokens or not label_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    precision = len(pred_tokens.intersection(label_tokens)) / len(pred_tokens)\n",
    "    recall = len(pred_tokens.intersection(label_tokens)) / len(label_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def extract_metrics(log_file_path):\n",
    "    \"\"\"\n",
    "    Extracts exact match, token F1, SBERT score, BLEU, and max token length from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing lists of exact match, token F1, SBERT score,\n",
    "        BLEU, max token length, and dataset name, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_line(line):\n",
    "        \"\"\"Removes 'Index X:' prefix and extra spacing from a line.\"\"\"\n",
    "        line = re.sub(r\"^Index \\d+: \", \"\", line)\n",
    "        line = line.strip()\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        line = re.sub(r\"\\s*/\\s*\", \"/\", line)\n",
    "        return line\n",
    "\n",
    "    preds = {}\n",
    "    labels = {}\n",
    "\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    preds_section = False\n",
    "    labels_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Contents of decoded_preds:\" in line:\n",
    "            preds_section = True\n",
    "            labels_section = False\n",
    "            continue\n",
    "        elif \"Contents of decoded_labels:\" in line:\n",
    "            preds_section = False\n",
    "            labels_section = True\n",
    "            continue\n",
    "\n",
    "        if preds_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                preds[index] = match.group(2)\n",
    "        elif labels_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                labels[index] = match.group(2)\n",
    "\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Warning: Number of predictions ({len(preds)}) and labels ({len(labels)}) differ in {log_file_path}.\")\n",
    "\n",
    "    min_len = min(len(preds), len(labels))\n",
    "\n",
    "    # Initialize Sentence-BERT model and SacreBLEU\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    bleu = load(\"sacrebleu\")\n",
    "\n",
    "    exact_matches = []\n",
    "    token_f1s = []\n",
    "    sbert_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    data_added_to_bleu = False  # Flag to track if data was added to bleu\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if i in preds and i in labels:\n",
    "            cleaned_pred = clean_line(preds[i])\n",
    "            cleaned_label = clean_line(labels[i])\n",
    "\n",
    "            # Exact Match\n",
    "            exact_match = 1.0 if cleaned_pred == cleaned_label else 0.0\n",
    "            exact_matches.append(exact_match)\n",
    "\n",
    "            # Token F1\n",
    "            token_f1 = calculate_token_f1(cleaned_pred, cleaned_label)\n",
    "            token_f1s.append(token_f1)\n",
    "\n",
    "            # Sentence-BERT\n",
    "            pred_embedding = model.encode(cleaned_pred, convert_to_tensor=True)\n",
    "            label_embedding = model.encode(cleaned_label, convert_to_tensor=True)\n",
    "            cosine_sim = util.pytorch_cos_sim(pred_embedding, label_embedding)\n",
    "            sbert_scores.append(cosine_sim.item())\n",
    "\n",
    "            # SacreBLEU\n",
    "            bleu.add_batch(predictions=[cleaned_pred], references=[[cleaned_label]])\n",
    "            data_added_to_bleu = True\n",
    "\n",
    "    # Compute BLEU only if data was added\n",
    "    if data_added_to_bleu:\n",
    "        bleu_results = bleu.compute()\n",
    "        bleu_scores = [bleu_results[\"score\"]] * min_len\n",
    "    else:\n",
    "        bleu_scores = [0.0] * min_len\n",
    "\n",
    "    # Extract max token and dataset name from filename\n",
    "    if \"maxtoken\" in log_file_path:\n",
    "        max_token = int(log_file_path.split(\"_\")[-1].replace(\"maxtoken.log\", \"\"))\n",
    "    else:\n",
    "        max_token = None\n",
    "\n",
    "    match = re.search(r\"_(.*?)_500samples_\", log_file_path)\n",
    "    dataset_name = match.group(1) if match else None\n",
    "\n",
    "    if max_token is not None and dataset_name is not None:\n",
    "        return {\n",
    "            \"max_token\": max_token,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"token_f1\": token_f1s,\n",
    "            \"sbert_score\": sbert_scores,  # Use sbert_score\n",
    "            \"bleu\": bleu_scores,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_metrics_vs_max_token(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Plots exact match, token F1, SBERT score, and BLEU against max token lengths for multiple datasets on the same plot.\n",
    "\n",
    "    Args:\n",
    "        all_data: A dictionary where keys are dataset names and values are dictionaries\n",
    "                  of max token lengths to lists of metrics.\n",
    "        output_dir: The directory to save the plots to.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": \"Exact Match\",\n",
    "        \"token_f1\": \"Token F1\",\n",
    "        \"sbert_score\": \"SBERT Score\",\n",
    "        \"bleu\": \"BLEU\"\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_label in metrics.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for dataset_name, data in all_data.items():\n",
    "            max_tokens = sorted(data.keys())\n",
    "            means = [np.mean(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "            stds = [np.std(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "\n",
    "            plt.errorbar(max_tokens, means, yerr=stds, fmt='o-', capsize=5, label=dataset_name)\n",
    "\n",
    "        plt.xlabel(\"Max Token Length\")\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.title(f\"{metric_label} vs. Max Token Length\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        output_path = os.path.join(output_dir, f\"{metric_name}_vs_max_token_all_datasets.png\")\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"{metric_label} plot saved to: {output_path}\")\n",
    "        plt.close()\n",
    "\n",
    "# --- Configuration ---\n",
    "log_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\"\n",
    "output_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "all_data = {}  # Dictionary to store data for all datasets\n",
    "\n",
    "for filename in os.listdir(log_dir):\n",
    "    if filename.endswith(\".log\"):\n",
    "        filepath = os.path.join(log_dir, filename)\n",
    "        result = extract_metrics(filepath)\n",
    "\n",
    "        if result:\n",
    "            max_token = result[\"max_token\"]\n",
    "            dataset_name = result[\"dataset_name\"]\n",
    "            if dataset_name not in all_data:\n",
    "                all_data[dataset_name] = {}\n",
    "            if max_token not in all_data[dataset_name]:\n",
    "                all_data[dataset_name][max_token] = {\n",
    "                    \"exact_match\": [],\n",
    "                    \"token_f1\": [],\n",
    "                    \"sbert_score\": [],  # Use sbert_score here\n",
    "                    \"bleu\": []\n",
    "                }\n",
    "            all_data[dataset_name][max_token][\"exact_match\"].extend(result[\"exact_match\"])\n",
    "            all_data[dataset_name][max_token][\"token_f1\"].extend(result[\"token_f1\"])\n",
    "            all_data[dataset_name][max_token][\"sbert_score\"].extend(result[\"sbert_score\"])  # Extend sbert_score\n",
    "            all_data[dataset_name][max_token][\"bleu\"].extend(result[\"bleu\"])\n",
    "\n",
    "if all_data:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_metrics_vs_max_token(all_data, output_dir)\n",
    "else:\n",
    "    print(\"No valid data found in log files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\n",
      "Output directory: /home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_token_f1(pred, label):\n",
    "    \"\"\"\n",
    "    Calculates the token-level F1 score between two strings.\n",
    "\n",
    "    Args:\n",
    "        pred: The predicted string.\n",
    "        label: The true label string.\n",
    "\n",
    "    Returns:\n",
    "        The token-level F1 score.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(pred.lower().split())\n",
    "    label_tokens = set(label.lower().split())\n",
    "\n",
    "    if not pred_tokens and not label_tokens:\n",
    "        return 1.0  # Both empty\n",
    "\n",
    "    if not pred_tokens or not label_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    precision = len(pred_tokens.intersection(label_tokens)) / len(pred_tokens)\n",
    "    recall = len(pred_tokens.intersection(label_tokens)) / len(label_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def extract_metrics(log_file_path):\n",
    "    \"\"\"\n",
    "    Extracts exact match, token F1, SBERT score, BLEU, and max token length from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing lists of exact match, token F1, SBERT score,\n",
    "        BLEU, max token length, and dataset name, or None if not found.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_line(line):\n",
    "        \"\"\"Removes 'Index X:' prefix and extra spacing from a line.\"\"\"\n",
    "        line = re.sub(r\"^Index \\d+: \", \"\", line)\n",
    "        line = line.strip()\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        line = re.sub(r\"\\s*/\\s*\", \"/\", line)\n",
    "        return line\n",
    "\n",
    "    preds = {}\n",
    "    labels = {}\n",
    "\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    preds_section = False\n",
    "    labels_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Contents of decoded_preds:\" in line:\n",
    "            preds_section = True\n",
    "            labels_section = False\n",
    "            continue\n",
    "        elif \"Contents of decoded_labels:\" in line:\n",
    "            preds_section = False\n",
    "            labels_section = True\n",
    "            continue\n",
    "\n",
    "        if preds_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                preds[index] = match.group(2)\n",
    "        elif labels_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                labels[index] = match.group(2)\n",
    "\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Warning: Number of predictions ({len(preds)}) and labels ({len(labels)}) differ in {log_file_path}.\")\n",
    "\n",
    "    min_len = min(len(preds), len(labels))\n",
    "\n",
    "    # Initialize Sentence-BERT model and SacreBLEU\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    bleu = load(\"sacrebleu\")\n",
    "\n",
    "    exact_matches = []\n",
    "    token_f1s = []\n",
    "    sbert_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    data_added_to_bleu = False  # Flag to track if data was added to bleu\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if i in preds and i in labels:\n",
    "            cleaned_pred = clean_line(preds[i])\n",
    "            cleaned_label = clean_line(labels[i])\n",
    "\n",
    "            # Exact Match\n",
    "            exact_match = 1.0 if cleaned_pred == cleaned_label else 0.0\n",
    "            exact_matches.append(exact_match)\n",
    "\n",
    "            # Token F1\n",
    "            token_f1 = calculate_token_f1(cleaned_pred, cleaned_label)\n",
    "            token_f1s.append(token_f1)\n",
    "\n",
    "            # Sentence-BERT\n",
    "            pred_embedding = model.encode(cleaned_pred, convert_to_tensor=True)\n",
    "            label_embedding = model.encode(cleaned_label, convert_to_tensor=True)\n",
    "            cosine_sim = util.pytorch_cos_sim(pred_embedding, label_embedding)\n",
    "            sbert_scores.append(cosine_sim.item())\n",
    "\n",
    "            # SacreBLEU\n",
    "            bleu.add_batch(predictions=[cleaned_pred], references=[[cleaned_label]])\n",
    "            data_added_to_bleu = True\n",
    "\n",
    "    # Compute BLEU only if data was added\n",
    "    if data_added_to_bleu:\n",
    "        bleu_results = bleu.compute()\n",
    "        bleu_scores = [bleu_results[\"score\"]] * min_len\n",
    "    else:\n",
    "        bleu_scores = [0.0] * min_len\n",
    "\n",
    "    # Extract max token and dataset name from filename\n",
    "    if \"maxtoken\" in log_file_path:\n",
    "        max_token = int(log_file_path.split(\"_\")[-1].replace(\"maxtoken.log\", \"\"))\n",
    "    else:\n",
    "        max_token = None\n",
    "\n",
    "    match = re.search(r\"_(.*?)_500samples_\", log_file_path)\n",
    "    dataset_name = match.group(1) if match else None\n",
    "\n",
    "    if max_token is not None and dataset_name is not None:\n",
    "        return {\n",
    "            \"max_token\": max_token,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"token_f1\": token_f1s,\n",
    "            \"sbert_score\": sbert_scores,  # Use sbert_score\n",
    "            \"bleu\": bleu_scores,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_metrics_vs_max_token(all_data, output_dir):\n",
    "    \"\"\"\n",
    "    Plots exact match, token F1, SBERT score, and BLEU against max token lengths for multiple datasets on the same plot.\n",
    "\n",
    "    Args:\n",
    "        all_data: A dictionary where keys are dataset names and values are dictionaries\n",
    "                  of max token lengths to lists of metrics.\n",
    "        output_dir: The directory to save the plots to.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"exact_match\": \"Exact Match\",\n",
    "        \"token_f1\": \"Token F1\",\n",
    "        \"sbert_score\": \"SBERT Score\",\n",
    "        \"bleu\": \"BLEU\"\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_label in metrics.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for dataset_name, data in all_data.items():\n",
    "            max_tokens = sorted(data.keys())\n",
    "            means = [np.mean(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "            stds = [np.std(data[max_token][metric_name]) for max_token in max_tokens]\n",
    "\n",
    "            plt.errorbar(max_tokens, means, yerr=stds, fmt='o-', capsize=5, label=dataset_name)\n",
    "\n",
    "        plt.xlabel(\"Max Token Length\")\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.title(f\"{metric_label} vs. Max Token Length\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        output_path = os.path.join(output_dir, f\"{metric_name}_vs_max_token_all_datasets.png\")\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"{metric_label} plot saved to: {output_path}\")\n",
    "        plt.close()\n",
    "\n",
    "# --- Configuration ---\n",
    "log_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/\"\n",
    "output_dir = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/plots/\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "all_data = {}  # Dictionary to store data for all datasets\n",
    "\n",
    "for filename in os.listdir(log_dir):\n",
    "    if filename.endswith(\".log\"):\n",
    "        filepath = os.path.join(log_dir, filename)\n",
    "        result = extract_metrics(filepath)\n",
    "\n",
    "        if result:\n",
    "            max_token = result[\"max_token\"]\n",
    "            dataset_name = result[\"dataset_name\"]\n",
    "            if dataset_name not in all_data:\n",
    "                all_data[dataset_name] = {}\n",
    "            if max_token not in all_data[dataset_name]:\n",
    "                all_data[dataset_name][max_token] = {\n",
    "                    \"exact_match\": [],\n",
    "                    \"token_f1\": [],\n",
    "                    \"sbert_score\": [],  # Use sbert_score here\n",
    "                    \"bleu\": []\n",
    "                }\n",
    "            all_data[dataset_name][max_token][\"exact_match\"].extend(result[\"exact_match\"])\n",
    "            all_data[dataset_name][max_token][\"token_f1\"].extend(result[\"token_f1\"])\n",
    "            all_data[dataset_name][max_token][\"sbert_score\"].extend(result[\"sbert_score\"])  # Extend sbert_score\n",
    "            all_data[dataset_name][max_token][\"bleu\"].extend(result[\"bleu\"])\n",
    "\n",
    "if all_data:\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_metrics_vs_max_token(all_data, output_dir)\n",
    "else:\n",
    "    print(\"No valid data found in log files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code investigates the different metrics on a dataset in a qualitative sense (we look at the samples to gauge how the metrics perform). Here you can notice certain metrics start performing worse on lower token lengths because of the way the metrics are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Index 0 ---\n",
      "Prediction: Aurelius Ambrosi\n",
      "Label:      Aurelius Ambrosi\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 1 ---\n",
      "Prediction: The Academy Awards or The Oscars\n",
      "Label:      The Academy Awards or The Oscars\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 2 ---\n",
      "Prediction: The Economy of Angola is\n",
      "Label:      The Economy of Angola is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 3 ---\n",
      "Prediction: The northern cavefish or northern blind\n",
      "Label:      The northern cavefish or northern blind\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 4 ---\n",
      "Prediction: Adelaide ( , \"Dale\n",
      "Label:      Adelaide (/dle\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0179, Recall: -0.0370, F1: -0.0080\n",
      "--- Index 5 ---\n",
      "Prediction: Actaeon (/\n",
      "Label:      Actaeon (/\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 6 ---\n",
      "Prediction: An aromatic hydrocarbon or aren\n",
      "Label:      An aromatic hydrocarbon or aren\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 7 ---\n",
      "Prediction: The Apollo program, also known as\n",
      "Label:      The Apollo program, also known as\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 8 ---\n",
      "Prediction: Mouthwash,\n",
      "Label:      Mouthwash, mouth rinse,\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5125, Recall: 0.1695, F1: 0.3365\n",
      "--- Index 9 ---\n",
      "Prediction: Abbot, meaning father, is\n",
      "Label:      Abbot, meaning father, is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 10 ---\n",
      "Prediction: Abensberg (German pronunciation\n",
      "Label:      Abensberg (German pronunciation\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 11 ---\n",
      "Prediction: Albert Schweitzer (14 January 18\n",
      "Label:      Albert Schweitzer (14 January 18\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 12 ---\n",
      "Prediction: Antonio Lucio Vivaldi\n",
      "Label:      Antonio Lucio Vivaldi\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 13 ---\n",
      "Prediction: Accordions (from 19th\n",
      "Label:      Accordions (from 19th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 14 ---\n",
      "Prediction: Americium is a radio\n",
      "Label:      Americium is a radio\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 15 ---\n",
      "Prediction: The Aare (German pronunciation:\n",
      "Label:      The Aare (German pronunciation:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 16 ---\n",
      "Prediction: Amyntas III (\n",
      "Label:      Amyntas III (\n",
      "Match: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse-wonnink/miniconda3/envs/MLjesse/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jesse-wonnink/miniconda3/envs/MLjesse/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 17 ---\n",
      "Prediction: The Alexandrists were a\n",
      "Label:      The Alexandrists were a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 18 ---\n",
      "Prediction: Aristotle (\n",
      "Label:      Aristotle (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.7311, Recall: 0.7311, F1: 0.7315\n",
      "--- Index 19 ---\n",
      "Prediction: Revich aevii Revich aevii, Alex Petro\n",
      "Label:      Alexei Petrovich Roma\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.2856, Recall: -0.1997, F1: -0.2410\n",
      "--- Index 20 ---\n",
      "Prediction: The Analytical Engine was\n",
      "Label:      The Analytical Engine was\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 21 ---\n",
      "Prediction: Anacharsis (/\n",
      "Label:      Anacharsis (/\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 22 ---\n",
      "Prediction: NYSE MKT LLC,\n",
      "Label:      NYSE MKT LLC,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 23 ---\n",
      "Prediction: Anarchism is a\n",
      "Label:      Anarchism is a\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 24 ---\n",
      "Prediction: ada King, ada Augusta ada King, ad\n",
      "Label:      Augusta Ada King,\n",
      "Match: No\n",
      "BLEU Score: 0.2985\n",
      "BERTScore - Precision: -0.1586, Recall: -0.0070, F1: -0.0822\n",
      "--- Index 25 ---\n",
      "Prediction: Assistive technology is an\n",
      "Label:      Assistive technology is an\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 26 ---\n",
      "Prediction: Alexander Balas (An\n",
      "Label:      Alexander Balas (An\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 27 ---\n",
      "Prediction: American Sign Language (ASL)\n",
      "Label:      American Sign Language (ASL)\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 28 ---\n",
      "Prediction: An algebraic number is a\n",
      "Label:      An algebraic number is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 29 ---\n",
      "Prediction: A (named a\n",
      "Label:      A (named a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 30 ---\n",
      "Prediction: In religion In mythology and philosophy\n",
      "Label:      In philosophy, religion, mythology\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4903, Recall: 0.5156, F1: 0.5037\n",
      "--- Index 31 ---\n",
      "Prediction: The Americans with Disabilities Act\n",
      "Label:      The Americans with Disabilities Act\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 32 ---\n",
      "Prediction: APL (named after the\n",
      "Label:      APL (named after the\n",
      "Match: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse-wonnink/miniconda3/envs/MLjesse/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 33 ---\n",
      "Prediction: Generally, an answer is\n",
      "Label:      Generally, an answer is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 34 ---\n",
      "Prediction: Area is the quantity that expresse\n",
      "Label:      Area is the quantity that expresse\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 35 ---\n",
      "Prediction: Asphalt (US)/s\n",
      "Label:      Asphalt (US/s\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6085, Recall: 0.6086, F1: 0.6092\n",
      "--- Index 36 ---\n",
      "Prediction: Arthur Schopenhauer (German\n",
      "Label:      Arthur Schopenhauer (German\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 37 ---\n",
      "Prediction: /Altai\n",
      "Label:      Altaic/l\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.0329, Recall: -0.3677, F1: -0.2042\n",
      "--- Index 38 ---\n",
      "Prediction: Albolin ( 530s)\n",
      "Label:      Alboin (530s\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.2905, Recall: -0.1079, F1: 0.0849\n",
      "--- Index 39 ---\n",
      "Prediction: Anaxarchus (\n",
      "Label:      Anaxarchus (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8854, Recall: 0.8855, F1: 0.8856\n",
      "--- Index 40 ---\n",
      "Prediction: Ahab (Hebrew:\n",
      "Label:      Ahab (Hebrew:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 41 ---\n",
      "Prediction: The Ashes is a\n",
      "Label:      The Ashes is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 42 ---\n",
      "Prediction: Ambiorix was,\n",
      "Label:      Ambiorix was, together with\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6953, Recall: 0.4490, F1: 0.5701\n",
      "--- Index 43 ---\n",
      "Prediction: The American Football Conference (AFC\n",
      "Label:      The American Football Conference (AFC\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 44 ---\n",
      "Prediction: Alexander I or Aleksandar\n",
      "Label:      Alexander I or Aleksandar\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 45 ---\n",
      "Prediction: Alfonso Cuar\n",
      "Label:      Alfonso Cuar\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 46 ---\n",
      "Prediction: An amateur (French amateur\n",
      "Label:      An amateur (French amateur\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 47 ---\n",
      "Prediction: Alger of Lige (1055\n",
      "Label:      Alger of Lige (1055\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 48 ---\n",
      "Prediction: The Apiaceae or\n",
      "Label:      The Apiaceae or\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 49 ---\n",
      "Prediction: Anositropy/\n",
      "Label:      Anisotropy/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5554, Recall: 0.2550, F1: 0.4019\n",
      "--- Index 50 ---\n",
      "Prediction: Anatoly ygaevsk Anatoly ygaevsk\n",
      "Label:      Anatoly Yevgeny\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.1882, Recall: 0.2932, F1: 0.2414\n",
      "--- Index 51 ---\n",
      "Prediction: Abraham Lincoln (\n",
      "Label:      Abraham Lincoln/e\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5156, Recall: 0.2128, F1: 0.3609\n",
      "--- Index 52 ---\n",
      "Prediction: Albert of Prussia (17 May 14\n",
      "Label:      Albert of Prussia (17 May 14\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 53 ---\n",
      "Prediction: Acute disseminated\n",
      "Label:      Acute disseminated\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 54 ---\n",
      "Prediction: Aztlan Underground is a\n",
      "Label:      Aztlan Underground is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 55 ---\n",
      "Prediction: An aircraft is a machine that\n",
      "Label:      An aircraft is a machine that\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 56 ---\n",
      "Prediction: In Norse mythology,\n",
      "Label:      In Norse mythology,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 57 ---\n",
      "Prediction: The abacus (plu\n",
      "Label:      The abacus (plu\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 58 ---\n",
      "Prediction: The MessagePad is the first\n",
      "Label:      The MessagePad is the first\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 59 ---\n",
      "Prediction: Alin of York Alin of New York cu\n",
      "Label:      Alcuin of York (La\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.0480, Recall: -0.1429, F1: -0.0941\n",
      "--- Index 60 ---\n",
      "Prediction: Ardipithecus is\n",
      "Label:      Ardipithecus is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 61 ---\n",
      "Prediction: Allan Dwan (3 April 18\n",
      "Label:      Allan Dwan (3 April 18\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 62 ---\n",
      "Prediction: Ajax or Aia\n",
      "Label:      Ajax or Aia\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 63 ---\n",
      "Prediction: The Andes is the longest\n",
      "Label:      The Andes is the longest\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 64 ---\n",
      "Prediction: Alexander Aetolus (A\n",
      "Label:      Alexander Aetolus (A\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 65 ---\n",
      "Prediction: Augustus (Latin:\n",
      "Label:      Augustus (Latin:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 66 ---\n",
      "Prediction: Aruba (\n",
      "Label:      Aruba (/ru\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6672, Recall: 0.3432, F1: 0.5012\n",
      "--- Index 67 ---\n",
      "Prediction: Asia (/////e\n",
      "Label:      Asia (/e/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3387, Recall: 0.5010, F1: 0.4196\n",
      "--- Index 68 ---\n",
      "Prediction: Alfonso Mal Alfonso the Magn,\n",
      "Label:      Alfonso the Magn\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3636, Recall: 0.6786, F1: 0.5173\n",
      "--- Index 69 ---\n",
      "Prediction: August 234th is\n",
      "Label:      August 22 is the 234th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4384, Recall: 0.3049, F1: 0.3718\n",
      "--- Index 70 ---\n",
      "Prediction: Antimony is a chemical element\n",
      "Label:      Antimony is a chemical element\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 71 ---\n",
      "Prediction: April 6 is the 96th\n",
      "Label:      April 6 is the 96th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 72 ---\n",
      "Prediction: Alan Alexander Milne (\n",
      "Label:      Alan Alexander Milne (/\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.8573, Recall: 0.8573, F1: 0.8576\n",
      "--- Index 73 ---\n",
      "Prediction: Alberta (\n",
      "Label:      Alberta (/lb\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5498, Recall: 0.3195, F1: 0.4331\n",
      "--- Index 74 ---\n",
      "Prediction: Ames is a city located\n",
      "Label:      Ames is a city located\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 75 ---\n",
      "Prediction: August 2 is the 214th\n",
      "Label:      August 2 is the 214th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 76 ---\n",
      "Prediction: Astrometry is the branch\n",
      "Label:      Astrometry is the branch of\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.7531, Recall: 0.6175, F1: 0.6850\n",
      "--- Index 77 ---\n",
      "Prediction: An assembled assembly language An assemble language ( ) or assembly\n",
      "Label:      An assembly language (or assemble\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.2765, Recall: 0.4089, F1: 0.3430\n",
      "--- Index 78 ---\n",
      "Prediction: Alaborg Municipality Alaborg Municipality ( ) is\n",
      "Label:      Aalborg Municipality is\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.2170, Recall: 0.2731, F1: 0.2461\n",
      "--- Index 79 ---\n",
      "Prediction: Agnosticism is the view\n",
      "Label:      Agnosticism is the view\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 80 ---\n",
      "Prediction: Animalia is an illustrated children\n",
      "Label:      Animalia is an illustrated children\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 81 ---\n",
      "Prediction: This article is about communications systems in\n",
      "Label:      This article is about communications systems in\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 82 ---\n",
      "Prediction: Alcidamas (Gr\n",
      "Label:      Alcidamas (Gr\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 83 ---\n",
      "Prediction: Analog Brothers were an experimental hip\n",
      "Label:      Analog Brothers were an experimental hip\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 84 ---\n",
      "Prediction: A Modest Proposal for\n",
      "Label:      A Modest Proposal for\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 85 ---\n",
      "Prediction: Amalaric (Goth\n",
      "Label:      Amalaric (Goth\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 86 ---\n",
      "Prediction: Andrey (Andrei\n",
      "Label:      Andrey (Andrei\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 87 ---\n",
      "Prediction: The Dodo is a\n",
      "Label:      The Dodo is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 88 ---\n",
      "Prediction: Alan Garner OBE (born\n",
      "Label:      Alan Garner OBE (born\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 89 ---\n",
      "Prediction: Angst means fear or anxiety (ang\n",
      "Label:      Angst means fear or anxiety (ang\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 90 ---\n",
      "Prediction: Aristophanes (\n",
      "Label:      Aristophanes (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8804, Recall: 0.8804, F1: 0.8806\n",
      "--- Index 91 ---\n",
      "Prediction: (Ambra\n",
      "Label:      Ambracia (/m\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0718, Recall: -0.1782, F1: -0.0547\n",
      "--- Index 92 ---\n",
      "Prediction: Albert II (German: Albrecht\n",
      "Label:      Albert II (German: Albrecht\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 93 ---\n",
      "Prediction: Alexander (Ancient Greek\n",
      "Label:      Alexander (Ancient Greek\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 94 ---\n",
      "Prediction: Armenia is the second most densely\n",
      "Label:      Armenia is the second most densely\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 95 ---\n",
      "Prediction: Alcamenes (A\n",
      "Label:      Alcamenes (A\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 96 ---\n",
      "Prediction: According to the Bible, Absal\n",
      "Label:      According to the Bible, Absal\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 97 ---\n",
      "Prediction: Allah (English pronunciation:/\n",
      "Label:      Allah (English pronunciation:/\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 98 ---\n",
      "Prediction: Audi AG (pronounced [a\n",
      "Label:      Audi AG (pronounced [a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 99 ---\n",
      "Prediction: August 21 is the 233r\n",
      "Label:      August 21 is the 233r\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 100 ---\n",
      "Prediction: An abbey (from Latin\n",
      "Label:      An abbey (from Latin\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 101 ---\n",
      "Prediction: Ankara (English)\n",
      "Label:      Ankara (English/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8637, Recall: 0.5210, F1: 0.6876\n",
      "--- Index 102 ---\n",
      "Prediction: Algorithms is\n",
      "Label:      Algorithms is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 103 ---\n",
      "Prediction: Agnostida is an order\n",
      "Label:      Agnostida is an order\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 104 ---\n",
      "Prediction: Absolute zero is the lower limit\n",
      "Label:      Absolute zero is the lower limit\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 105 ---\n",
      "Prediction: Anthropology is the study of humanity\n",
      "Label:      Anthropology is the study of humanity\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 106 ---\n",
      "Prediction: August 226 is the 14th\n",
      "Label:      August 14 is the 226th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5812, Recall: 0.5603, F1: 0.5714\n",
      "--- Index 107 ---\n",
      "Prediction: The Park Gell (Cat\n",
      "Label:      The Park Gell (Cat\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 108 ---\n",
      "Prediction: An artist is a person engaged\n",
      "Label:      An artist is a person engaged\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 109 ---\n",
      "Prediction: AWK is an interpreted\n",
      "Label:      AWK is an interpreted\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 110 ---\n",
      "Prediction: The ampere (SI\n",
      "Label:      The ampere (SI\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 111 ---\n",
      "Prediction: Athena (/i\n",
      "Label:      Athena (/i\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 112 ---\n",
      "Prediction: Dame Agatha Mary\n",
      "Label:      Dame Agatha Mary\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 113 ---\n",
      "Prediction: An anaconda is\n",
      "Label:      An anaconda is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 114 ---\n",
      "Prediction: Apple Computer 1 , also known retro\n",
      "Label:      Apple Computer 1, also known retroactive\n",
      "Match: No\n",
      "BLEU Score: 0.8091\n",
      "BERTScore - Precision: 0.6136, Recall: 0.5009, F1: 0.5574\n",
      "--- Index 115 ---\n",
      "Prediction: Alcaeus of mytile\n",
      "Label:      Alcaeus of Mytile\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.2986, Recall: 0.2115, F1: 0.2559\n",
      "--- Index 116 ---\n",
      "Prediction: Abraxas (Gk\n",
      "Label:      Abraxas (Gk\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 117 ---\n",
      "Prediction: Anatole France (Fre\n",
      "Label:      Anatole France (Fre\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 118 ---\n",
      "Prediction: A Clockwork Orange is\n",
      "Label:      A Clockwork Orange is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 119 ---\n",
      "Prediction: Arminianism is based\n",
      "Label:      Arminianism is based\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 120 ---\n",
      "Prediction: An army (from Latin arma\n",
      "Label:      An army (from Latin arma\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 121 ---\n",
      "Prediction: Alfred Russel Wallace OM F\n",
      "Label:      Alfred Russel Wallace OM F\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 122 ---\n",
      "Prediction: Afro Celt Sound System is\n",
      "Label:      Afro Celt Sound System is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 123 ---\n",
      "Prediction: Alessandro algari Alessandro algari, a.k.a Algard\n",
      "Label:      Alessandro Algardi\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.1967, Recall: 0.5291, F1: 0.3587\n",
      "--- Index 124 ---\n",
      "Prediction: The Aegean Sea (\n",
      "Label:      The Aegean Sea (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 125 ---\n",
      "Prediction: Asterales//s\n",
      "Label:      Asterales/s\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6633, Recall: 0.6634, F1: 0.6639\n",
      "--- Index 126 ---\n",
      "Prediction: Since its independence , Armenia\n",
      "Label:      Since its independence, Armenia has maintained\n",
      "Match: No\n",
      "BLEU Score: 0.6703\n",
      "BERTScore - Precision: 0.5721, Recall: 0.3612, F1: 0.4655\n",
      "--- Index 127 ---\n",
      "Prediction: Art is a diverse range of\n",
      "Label:      Art is a diverse range of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 128 ---\n",
      "Prediction: (Alexander of L\n",
      "Label:      Alexander (/lz\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.2051, Recall: -0.2466, F1: -0.2239\n",
      "--- Index 129 ---\n",
      "Prediction: Aberdeenshire (Scottish Ga\n",
      "Label:      Aberdeenshire (Scottish Ga\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 130 ---\n",
      "Prediction: The Antarctic Treaty and related agreements\n",
      "Label:      The Antarctic Treaty and related agreements\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 131 ---\n",
      "Prediction: Alain Connes (F\n",
      "Label:      Alain Connes (F\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 132 ---\n",
      "Prediction: August 17th is 29\n",
      "Label:      August 17 is the 229th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5154, Recall: 0.4450, F1: 0.4808\n",
      "--- Index 133 ---\n",
      "Prediction: Ambrosians are members of\n",
      "Label:      Ambrosians are members of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 134 ---\n",
      "Prediction: The Amber Diceless Role\n",
      "Label:      The Amber Diceless Role\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 135 ---\n",
      "Prediction: The almond (//m\n",
      "Label:      The almond (/m\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6542, Recall: 0.7972, F1: 0.7252\n",
      "--- Index 136 ---\n",
      "Prediction: August 9 is the 221s\n",
      "Label:      August 9 is the 221s\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 137 ---\n",
      "Prediction: Aluminium (see aluminum or different\n",
      "Label:      Aluminium (or aluminum; see different\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5268, Recall: 0.3878, F1: 0.4573\n",
      "--- Index 138 ---\n",
      "Prediction: Transport in Antarctica has transformed from\n",
      "Label:      Transport in Antarctica has transformed from\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 139 ---\n",
      "Prediction: The Austroasiatic languages,\n",
      "Label:      The Austroasiatic languages,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 140 ---\n",
      "Prediction: Afonso IV (\n",
      "Label:      Afonso IV (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 141 ---\n",
      "Prediction: Asociacin Alumni, usually\n",
      "Label:      Asociacin Alumni, usually\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 142 ---\n",
      "Prediction: Amethyst is\n",
      "Label:      Amethyst is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 143 ---\n",
      "Prediction: The Book of Amos is\n",
      "Label:      The Book of Amos is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 144 ---\n",
      "Prediction: An acropolis (G\n",
      "Label:      An acropolis (G\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 145 ---\n",
      "Prediction: Alexander of Aphrodisi\n",
      "Label:      Alexander of Aphrodisi\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 146 ---\n",
      "Prediction: Anna Sergeyevna\n",
      "Label:      Anna Sergeyevna\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 147 ---\n",
      "Prediction: Not to be confused with Amat\n",
      "Label:      Not to be confused with Amat\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 148 ---\n",
      "Prediction: In Greek mythology, Achilles\n",
      "Label:      In Greek mythology, Achilles\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 149 ---\n",
      "Prediction: Apple Inc. (commonly\n",
      "Label:      Apple Inc. (commonly\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 150 ---\n",
      "Prediction: Apollo (Attic, I\n",
      "Label:      Apollo (Attic, I\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 151 ---\n",
      "Prediction: Sir Sultan Muhammed Shah,\n",
      "Label:      Sir Sultan Muhammed Shah,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 152 ---\n",
      "Prediction: ABBA (sty\n",
      "Label:      ABBA (sty\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 153 ---\n",
      "Prediction: Arthur Aikin, FLS\n",
      "Label:      Arthur Aikin, FLS\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 154 ---\n",
      "Prediction: Alfonso II (7\n",
      "Label:      Alfonso II (7\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 155 ---\n",
      "Prediction: Avicenna (\n",
      "Label:      Avicenna (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8166, Recall: 0.8166, F1: 0.8169\n",
      "--- Index 156 ---\n",
      "Prediction: The Atlantic Ocean is the second largest\n",
      "Label:      The Atlantic Ocean is the second largest\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 157 ---\n",
      "Prediction: ako Jogatini ako Alexander I (ako Joga\n",
      "Label:      Alexander I Jagiellon (\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.2006, Recall: 0.0356, F1: -0.0836\n",
      "--- Index 158 ---\n",
      "Prediction: An architect is a person who\n",
      "Label:      An architect is a person who\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 159 ---\n",
      "Prediction: Artificial intelligence (AI) is the\n",
      "Label:      Artificial intelligence (AI) is the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 160 ---\n",
      "Prediction: Abbotsford is a\n",
      "Label:      Abbotsford is a\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 161 ---\n",
      "Prediction: Aquaculture , also known\n",
      "Label:      Aquaculture, also known as aqua\n",
      "Match: No\n",
      "BLEU Score: 0.6065\n",
      "BERTScore - Precision: 0.5728, Recall: 0.3298, F1: 0.4495\n",
      "--- Index 162 ---\n",
      "Prediction: An atomic orbital is\n",
      "Label:      An atomic orbital is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 163 ---\n",
      "Prediction: An astronaut ( ), or cosmo\n",
      "Label:      An astronaut (or cosmo\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3936, Recall: 0.5230, F1: 0.4584\n",
      "--- Index 164 ---\n",
      "Prediction: The anthophytes were\n",
      "Label:      The anthophytes were\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 165 ---\n",
      "Prediction: Aphrodite (/f\n",
      "Label:      Aphrodite (/f\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 166 ---\n",
      "Prediction: American Samoa has a\n",
      "Label:      American Samoa has a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 167 ---\n",
      "Prediction: In the Book of Samuel, Ab\n",
      "Label:      In the Book of Samuel, Ab\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 168 ---\n",
      "Prediction: (August/St\n",
      "Label:      August (/st/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.1557, Recall: 0.0411, F1: 0.0992\n",
      "--- Index 169 ---\n",
      "Prediction: Vipina Srpania Vipina Srpania Agrippin\n",
      "Label:      Vipsania Agrippin\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.0203, Recall: 0.3916, F1: 0.1787\n",
      "--- Index 170 ---\n",
      "Prediction: The Apache Software Foundation/\n",
      "Label:      The Apache Software Foundation/\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 171 ---\n",
      "Prediction: August 16 is the 228th\n",
      "Label:      August 16 is the 228th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 172 ---\n",
      "Prediction: Asparagales is the name\n",
      "Label:      Asparagales is the name\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 173 ---\n",
      "Prediction: Azerbaijan is a\n",
      "Label:      Azerbaijan is a\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 174 ---\n",
      "Prediction: April 13 is the 103r\n",
      "Label:      April 13 is the 103r\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 175 ---\n",
      "Prediction: Hercule Poirot (\n",
      "Label:      Hercule Poirot (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 176 ---\n",
      "Prediction: Alexander III of Macedon (20/21\n",
      "Label:      Alexander III of Macedon (20/21\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 177 ---\n",
      "Prediction: Alabama (Alabama: lb\n",
      "Label:      Alabama (/lb\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0732, Recall: 0.2869, F1: 0.1792\n",
      "--- Index 178 ---\n",
      "Prediction: Advanced Chemistry is a German hip\n",
      "Label:      Advanced Chemistry is a German hip\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 179 ---\n",
      "Prediction: Acacia (\n",
      "Label:      Acacia (/ke\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5692, Recall: 0.3157, F1: 0.4404\n",
      "--- Index 180 ---\n",
      "Prediction: Algol (Beta Per\n",
      "Label:      Algol (Beta Per\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 181 ---\n",
      "Prediction: Al Arslan Al Arslan Alp\n",
      "Label:      Alp Arslan (T\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.2871, Recall: 0.1744, F1: 0.2314\n",
      "--- Index 182 ---\n",
      "Prediction: American Samoa, located within\n",
      "Label:      American Samoa, located within\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 183 ---\n",
      "Prediction: August William Derleth (February\n",
      "Label:      August William Derleth (February\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 184 ---\n",
      "Prediction: Alan Curtis Kay (born\n",
      "Label:      Alan Curtis Kay (born\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 185 ---\n",
      "Prediction: Autism and Autism Autism is a neurodevelopmental\n",
      "Label:      Autism is a neurodevelopmental\n",
      "Match: No\n",
      "BLEU Score: 0.4111\n",
      "BERTScore - Precision: 0.4298, Recall: 0.6681, F1: 0.5471\n",
      "--- Index 186 ---\n",
      "Prediction: Arabic/Arabic: (R\n",
      "Label:      Arabic/rb\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.4562, Recall: -0.3845, F1: -0.4184\n",
      "--- Index 187 ---\n",
      "Prediction: The American Revolutionary War\n",
      "Label:      The American Revolutionary War (1775\n",
      "Match: No\n",
      "BLEU Score: 0.6065\n",
      "BERTScore - Precision: 0.9173, Recall: 0.5019, F1: 0.7025\n",
      "--- Index 188 ---\n",
      "Prediction: Alfons Maria Ja\n",
      "Label:      Alfons Maria Jak\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8211, Recall: 0.8211, F1: 0.8214\n",
      "--- Index 189 ---\n",
      "Prediction: Ailanthus (\n",
      "Label:      Ailanthus (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8828, Recall: 0.8828, F1: 0.8830\n",
      "--- Index 190 ---\n",
      "Prediction: An affidavit\n",
      "Label:      An affidavit\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 191 ---\n",
      "Prediction: The economy of American Samoa\n",
      "Label:      The economy of American Samoa\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 192 ---\n",
      "Prediction: Alexander I of Epirus (A\n",
      "Label:      Alexander I of Epirus (A\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 193 ---\n",
      "Prediction: Andy Warhol (/w\n",
      "Label:      Andy Warhol (/w\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 194 ---\n",
      "Prediction: Alredus, or of\n",
      "Label:      Alredus, or Alfred of\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5678, Recall: 0.4150, F1: 0.4911\n",
      "--- Index 195 ---\n",
      "Prediction: April 15 is the 105th\n",
      "Label:      April 15 is the 105th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 196 ---\n",
      "Prediction: An author is broadly defined as \"\n",
      "Label:      An author is broadly defined as \"\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 197 ---\n",
      "Prediction: Andre Kirk Agassi (\n",
      "Label:      Andre Kirk Agassi (/\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.8152, Recall: 0.8152, F1: 0.8155\n",
      "--- Index 198 ---\n",
      "Prediction: Alfheim (Old\n",
      "Label:      Alfheim (Old\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 199 ---\n",
      "Prediction: Amines (US:\n",
      "Label:      Amines (US:/\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.8025, Recall: 0.8025, F1: 0.8029\n",
      "--- Index 200 ---\n",
      "Prediction: In the Book of Exodus\n",
      "Label:      In the Book of Exodus\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 201 ---\n",
      "Prediction: In law, an abstract is\n",
      "Label:      In law, an abstract is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 202 ---\n",
      "Prediction: Ahmed I (Ottoman\n",
      "Label:      Ahmed I (Ottoman\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 203 ---\n",
      "Prediction: Algeria (Arabic:\n",
      "Label:      Algeria (Arabic:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 204 ---\n",
      "Prediction: An American in Paris is a\n",
      "Label:      An American in Paris is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 205 ---\n",
      "Prediction: This article is about the demographic features\n",
      "Label:      This article is about the demographic features\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 206 ---\n",
      "Prediction: April 16 is the 106th\n",
      "Label:      April 16 is the 106th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 207 ---\n",
      "Prediction: Sir Alfred Joseph Hitchcock,\n",
      "Label:      Sir Alfred Joseph Hitchcock,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 208 ---\n",
      "Prediction: August 1 is the 213th\n",
      "Label:      August 1 is the 213th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 209 ---\n",
      "Prediction: In algorithmic information theory (a\n",
      "Label:      In algorithmic information theory (a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 210 ---\n",
      "Prediction: April is the fourth month of the\n",
      "Label:      April is the fourth month of the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 211 ---\n",
      "Prediction: The Apiales are an order\n",
      "Label:      The Apiales are an order\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 212 ---\n",
      "Prediction: Alexander I (medieval Ga\n",
      "Label:      Alexander I (medieval Ga\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 213 ---\n",
      "Prediction: Asteroids is an arcade space\n",
      "Label:      Asteroids is an arcade space\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 214 ---\n",
      "Prediction: Alain de Lille (or\n",
      "Label:      Alain de Lille (or\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 215 ---\n",
      "Prediction: This article is about the demographic features\n",
      "Label:      This article is about the demographic features\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 216 ---\n",
      "Prediction: Albedo ( l\n",
      "Label:      Albedo (/l\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6597, Recall: 0.6597, F1: 0.6602\n",
      "--- Index 217 ---\n",
      "Prediction: The Asteraceae or\n",
      "Label:      The Asteraceae or\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 218 ---\n",
      "Prediction: Anarcho-capitalism\n",
      "Label:      Anarcho-capitalism\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 219 ---\n",
      "Prediction: An analogue or analog signal\n",
      "Label:      An analog or analogue signal\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.9506, Recall: 0.9507, F1: 0.9507\n",
      "--- Index 220 ---\n",
      "Prediction: Azincourt (French\n",
      "Label:      Azincourt (French\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 221 ---\n",
      "Prediction: Ada is a structured,\n",
      "Label:      Ada is a structured,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 222 ---\n",
      "Prediction: Andr Paul Guillaume Gide (F\n",
      "Label:      Andr Paul Guillaume Gide (F\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 223 ---\n",
      "Prediction: April 12 is the 102nd\n",
      "Label:      April 12 is the 102nd\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 224 ---\n",
      "Prediction: In chemistry and physics,\n",
      "Label:      In chemistry and physics,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 225 ---\n",
      "Prediction: Casa Batll (Cata\n",
      "Label:      Casa Batll (Cata\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 226 ---\n",
      "Prediction: August 23rd is the\n",
      "Label:      August 23 is the 235th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6090, Recall: 0.4247, F1: 0.5160\n",
      "--- Index 227 ---\n",
      "Prediction: Apollo 8, the second human spacef\n",
      "Label:      Apollo 8, the second human spacef\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 228 ---\n",
      "Prediction: An alloy is a mixture of\n",
      "Label:      An alloy is a mixture of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 229 ---\n",
      "Prediction: elit Vuelton elit Vuelton Alfred van\n",
      "Label:      Alfred Elton van Vogt\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.1530, Recall: 0.0695, F1: -0.0426\n",
      "--- Index 230 ---\n",
      "Prediction: Kaje ainser Kaje ainser, Ar\n",
      "Label:      Arne Kaijser\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.3798, Recall: -0.4844, F1: -0.4304\n",
      "--- Index 231 ---\n",
      "Prediction: An adventure is an exciting or unusual\n",
      "Label:      An adventure is an exciting or unusual\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 232 ---\n",
      "Prediction: Arys Arys is one of the zodiac\n",
      "Label:      Aries is one of the constellation\n",
      "Match: No\n",
      "BLEU Score: 0.4111\n",
      "BERTScore - Precision: 0.2318, Recall: 0.4066, F1: 0.3188\n",
      "--- Index 233 ---\n",
      "Prediction: Azerbaijan has an economy\n",
      "Label:      Azerbaijan has an economy\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 234 ---\n",
      "Prediction: Arianism is a non\n",
      "Label:      Arianism is a non\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 235 ---\n",
      "Prediction: April 22 is the 112th\n",
      "Label:      April 22 is the 112th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 236 ---\n",
      "Prediction: Amos Bronson Alcott\n",
      "Label:      Amos Bronson Alcott\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 237 ---\n",
      "Prediction: Aikido (Japa\n",
      "Label:      Aikido (Japa\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 238 ---\n",
      "Prediction: Azerbaijan is situated in\n",
      "Label:      Azerbaijan is situated in\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 239 ---\n",
      "Prediction: Jane Marple, usually referred\n",
      "Label:      Jane Marple, usually referred\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 240 ---\n",
      "Prediction: Anales School The Anales Fra (Prep),\n",
      "Label:      The Annales School (Fre\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.1798, Recall: 0.2511, F1: 0.2165\n",
      "--- Index 241 ---\n",
      "Prediction: Andrei Arsenyev\n",
      "Label:      Andrei Arsenyev\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 242 ---\n",
      "Prediction: In particle physics, antim\n",
      "Label:      In particle physics, antim\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 243 ---\n",
      "Prediction: Agate (//t\n",
      "Label:      Agate/t/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3089, Recall: 0.2664, F1: 0.2887\n",
      "--- Index 244 ---\n",
      "Prediction: Arsenic is a\n",
      "Label:      Arsenic is a\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 245 ---\n",
      "Prediction: August 12th is the 224\n",
      "Label:      August 12 is the 224th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5342, Recall: 0.4981, F1: 0.5169\n",
      "--- Index 246 ---\n",
      "Prediction: Attila (\n",
      "Label:      Attila (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6438, Recall: 0.6439, F1: 0.6444\n",
      "--- Index 247 ---\n",
      "Prediction: Alvin Toffler (born\n",
      "Label:      Alvin Toffler (born\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 248 ---\n",
      "Prediction: Anadyr (Russian\n",
      "Label:      Anadyr (Russian\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 249 ---\n",
      "Prediction: All Souls' Day, in\n",
      "Label:      All Souls' Day, in\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 250 ---\n",
      "Prediction: \"The Triumph of Time\" is\n",
      "Label:      \"The Triumph of Time\" is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 251 ---\n",
      "Prediction: Akira Kurosawa (\n",
      "Label:      Akira Kurosawa (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 252 ---\n",
      "Prediction: Alfred Bernhard Nobel (/n\n",
      "Label:      Alfred Bernhard Nobel (/n\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 253 ---\n",
      "Prediction: An allocution or all\n",
      "Label:      An allocution or all\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 254 ---\n",
      "Prediction: August 6 is the 218th\n",
      "Label:      August 6 is the 218th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 255 ---\n",
      "Prediction: August 236th is the 24\n",
      "Label:      August 24 is the 236th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4970, Recall: 0.4970, F1: 0.4978\n",
      "--- Index 256 ---\n",
      "Prediction: August Wilhelm Ambros (17\n",
      "Label:      August Wilhelm Ambros (17 November\n",
      "Match: No\n",
      "BLEU Score: 0.8187\n",
      "BERTScore - Precision: 0.8085, Recall: 0.6840, F1: 0.7460\n",
      "--- Index 257 ---\n",
      "Prediction: April 29th is 11\n",
      "Label:      April 29 is the 119th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4599, Recall: 0.3683, F1: 0.4147\n",
      "--- Index 258 ---\n",
      "Prediction: Albert Camus (French\n",
      "Label:      Albert Camus (French\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 259 ---\n",
      "Prediction: Animism (from Latin\n",
      "Label:      Animism (from Latin\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 260 ---\n",
      "Prediction: Afonso V (\n",
      "Label:      Afonso V (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 261 ---\n",
      "Prediction: Ataxia (from Greek\n",
      "Label:      Ataxia (from Greek\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 262 ---\n",
      "Prediction: This article is about communications systems in\n",
      "Label:      This article is about communications systems in\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 263 ---\n",
      "Prediction: The American Civil War, widely known\n",
      "Label:      The American Civil War, widely known\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 264 ---\n",
      "Prediction: (ASCI\n",
      "Label:      ASCII (/ski\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.2304, Recall: 0.1756, F1: -0.0341\n",
      "--- Index 265 ---\n",
      "Prediction: Alfred Skardank Habbe\n",
      "Label:      Alfred Habdank Skarbe\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5531, Recall: 0.4628, F1: 0.5083\n",
      "--- Index 266 ---\n",
      "Prediction: Adobe/(Do\n",
      "Label:      Adobe (/do\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5150, Recall: 0.5150, F1: 0.5158\n",
      "--- Index 267 ---\n",
      "Prediction: Afonso VI (\n",
      "Label:      Afonso VI (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 268 ---\n",
      "Prediction: The politics of Antigua and\n",
      "Label:      The politics of Antigua and\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 269 ---\n",
      "Prediction: Augustin-Jean Fres\n",
      "Label:      Augustin-Jean Fres\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 270 ---\n",
      "Prediction: April 30 is the 120th day\n",
      "Label:      April 30 is the 120th day\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 271 ---\n",
      "Prediction: April 1 is the 91s\n",
      "Label:      April 1 is the 91s\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 272 ---\n",
      "Prediction: Arable land (from Latin ar\n",
      "Label:      Arable land (from Latin ar\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 273 ---\n",
      "Prediction: (Abraham, Abraham\n",
      "Label:      Abraham (/eb\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.2734, Recall: -0.0230, F1: -0.1496\n",
      "--- Index 274 ---\n",
      "Prediction: The alphorn or alpen\n",
      "Label:      The alphorn or alpen\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 275 ---\n",
      "Prediction: August 19 is the 231s\n",
      "Label:      August 19 is the 231s\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 276 ---\n",
      "Prediction: The astronomical unit (s\n",
      "Label:      The astronomical unit (s\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 277 ---\n",
      "Prediction: August 13 is the 225th\n",
      "Label:      August 13 is the 225th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 278 ---\n",
      "Prediction: April 28 is the 118th\n",
      "Label:      April 28 is the 118th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 279 ---\n",
      "Prediction: Aga Khan I (Persian\n",
      "Label:      Aga Khan I (Persian\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 280 ---\n",
      "Prediction: The Territory of Ashmore and Car\n",
      "Label:      The Territory of Ashmore and Car\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 281 ---\n",
      "Prediction: Alexander III (Medieval\n",
      "Label:      Alexander III (Medieval\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 282 ---\n",
      "Prediction: For the (abbr.)\n",
      "Label:      For the abbr>\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3957, Recall: 0.4513, F1: 0.4243\n",
      "--- Index 283 ---\n",
      "Prediction: In Christianity , an abbes\n",
      "Label:      In Christianity, an abbes\n",
      "Match: No\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 0.7292, Recall: 0.7292, F1: 0.7296\n",
      "--- Index 284 ---\n",
      "Prediction: /Abalone (ba\n",
      "Label:      Abalone (/b\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3792, Recall: 0.4302, F1: 0.4055\n",
      "--- Index 285 ---\n",
      "Prediction: In mathematics, an automorphis\n",
      "Label:      In mathematics, an automorphis\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 286 ---\n",
      "Prediction: The Anglican Communion is\n",
      "Label:      The Anglican Communion is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 287 ---\n",
      "Prediction: August 239th is the 27\n",
      "Label:      August 27 is the 239th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4973, Recall: 0.4940, F1: 0.4965\n",
      "--- Index 288 ---\n",
      "Prediction: Amalric of Bena (\n",
      "Label:      Amalric of Bena (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 289 ---\n",
      "Prediction: An atom is the smallest\n",
      "Label:      An atom is the smallest\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 290 ---\n",
      "Prediction: In mathematics, the absolute value (\n",
      "Label:      In mathematics, the absolute value (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 291 ---\n",
      "Prediction: Azerbaijan (\n",
      "Label:      Azerbaijan (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.7316, Recall: 0.7316, F1: 0.7320\n",
      "--- Index 292 ---\n",
      "Prediction: August 18 is the 230th\n",
      "Label:      August 18 is the 230th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 293 ---\n",
      "Prediction: Politics of American Samoa\n",
      "Label:      Politics of American Samoa\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 294 ---\n",
      "Prediction: An alphabet is a standard set\n",
      "Label:      An alphabet is a standard set\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 295 ---\n",
      "Prediction: Amaranthus, collectively\n",
      "Label:      Amaranthus, collectively\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 296 ---\n",
      "Prediction: The Azerbaijani Arme\n",
      "Label:      The Azerbaijani Arme\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 297 ---\n",
      "Prediction: Agathon (an exonym:/\n",
      "Label:      Agathon (/n/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0405, Recall: 0.3063, F1: 0.1713\n",
      "--- Index 298 ---\n",
      "Prediction: Amino acids (\n",
      "Label:      Amino acids (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8869, Recall: 0.8869, F1: 0.8871\n",
      "--- Index 299 ---\n",
      "Prediction: This page lists some links to ancient\n",
      "Label:      This page lists some links to ancient\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 300 ---\n",
      "Prediction: August 7 is the 219th\n",
      "Label:      August 7 is the 219th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 301 ---\n",
      "Prediction: In mathematics and statistics, the\n",
      "Label:      In mathematics and statistics, the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 302 ---\n",
      "Prediction: Actrius: (Cata\n",
      "Label:      Actrius (Catalan:\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4471, Recall: 0.3594, F1: 0.4039\n",
      "--- Index 303 ---\n",
      "Prediction: Casa Mil (Catalan\n",
      "Label:      Casa Mil (Catalan\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 304 ---\n",
      "Prediction: Alismatales is an order\n",
      "Label:      Alismatales is an order\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 305 ---\n",
      "Prediction: The demographics of Armenia is about\n",
      "Label:      The demographics of Armenia is about\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 306 ---\n",
      "Prediction: Alan Mathison, Turing\n",
      "Label:      Alan Mathison Turing,\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4139, Recall: 0.3875, F1: 0.4016\n",
      "--- Index 307 ---\n",
      "Prediction: An axon ( from\n",
      "Label:      An axon (from\n",
      "Match: No\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 0.8306, Recall: 0.8306, F1: 0.8308\n",
      "--- Index 308 ---\n",
      "Prediction: An argot (English pronunciation:\n",
      "Label:      An argot (English pronunciation:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 309 ---\n",
      "Prediction: Albert the Bear (German: Alb\n",
      "Label:      Albert the Bear (German: Alb\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 310 ---\n",
      "Prediction: In common law, assault is the\n",
      "Label:      In common law, assault is the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 311 ---\n",
      "Prediction: The Amazing Spider-Man (abb\n",
      "Label:      The Amazing Spider-Man (abb\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 312 ---\n",
      "Prediction: August 8 is the 220th\n",
      "Label:      August 8 is the 220th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 313 ---\n",
      "Prediction: The foreign relations of Angola\n",
      "Label:      The foreign relations of Angola\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 314 ---\n",
      "Prediction: Actinium is a radio\n",
      "Label:      Actinium is a radio\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 315 ---\n",
      "Prediction: Anatolia (from Greek\n",
      "Label:      Anatolia (from Greek\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 316 ---\n",
      "Prediction: Ambrose Traversari,\n",
      "Label:      Ambrose Traversari,\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 317 ---\n",
      "Prediction: Eadld Eadld ( ), or Al\n",
      "Label:      Ealdred (or Al\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.1609, Recall: 0.0379, F1: -0.0618\n",
      "--- Index 318 ---\n",
      "Prediction: An abjad is a\n",
      "Label:      An abjad is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 319 ---\n",
      "Prediction: August 215th is\n",
      "Label:      August 3 is the 215th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4610, Recall: 0.2804, F1: 0.3702\n",
      "--- Index 320 ---\n",
      "Prediction: Ancient Egypt was a civilization of\n",
      "Label:      Ancient Egypt was a civilization of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 321 ---\n",
      "Prediction: Antonio Agliardi (4\n",
      "Label:      Antonio Agliardi (4 September\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.8029, Recall: 0.6776, F1: 0.7400\n",
      "--- Index 322 ---\n",
      "Prediction: In abstract algebra, an algebraically\n",
      "Label:      In abstract algebra, an algebraically\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 323 ---\n",
      "Prediction: In mathematics, the phrase \"al\n",
      "Label:      In mathematics, the phrase \"al\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 324 ---\n",
      "Prediction: ako II ako II ako II ako Kom\n",
      "Label:      Alexios II Komn\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.4665, Recall: -0.2657, F1: -0.3661\n",
      "--- Index 325 ---\n",
      "Prediction: Agriculture is the cultivation of animals,\n",
      "Label:      Agriculture is the cultivation of animals,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 326 ---\n",
      "Prediction: Kim Renard (born\n",
      "Label:      Kim Renard Nazel (born\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.7871, Recall: 0.5762, F1: 0.6802\n",
      "--- Index 327 ---\n",
      "Prediction: Julia Agrippina, most\n",
      "Label:      Julia Agrippina, most\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 328 ---\n",
      "Prediction: Cardinal Albert of Brandenburg (German\n",
      "Label:      Cardinal Albert of Brandenburg (German\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 329 ---\n",
      "Prediction: Aquarius is a constellation of\n",
      "Label:      Aquarius is a constellation of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 330 ---\n",
      "Prediction: Amasis II (A\n",
      "Label:      Amasis II (A\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 331 ---\n",
      "Prediction: Alexis (Greek:\n",
      "Label:      Alexis (Greek:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 332 ---\n",
      "Prediction: In Greco-Roman mythology\n",
      "Label:      In Greco-Roman mythology\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 333 ---\n",
      "Prediction: American Chinese cuisine, known in the\n",
      "Label:      American Chinese cuisine, known in the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 334 ---\n",
      "Prediction: Agrarianism has two common\n",
      "Label:      Agrarianism has two common\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 335 ---\n",
      "Prediction: The AK-47 (also\n",
      "Label:      The AK-47 (also\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 336 ---\n",
      "Prediction: The Governor of Alabama is the chief\n",
      "Label:      The Governor of Alabama is the chief\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 337 ---\n",
      "Prediction: In organic chemistry, an al\n",
      "Label:      In organic chemistry, an al\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 338 ---\n",
      "Prediction: This article considers transport in Armenia\n",
      "Label:      This article considers transport in Armenia\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 339 ---\n",
      "Prediction: The Advanced Encryption Standard\n",
      "Label:      The Advanced Encryption Standard\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 340 ---\n",
      "Prediction: \"Alga\" redirect\n",
      "Label:      \"Alga\" redirect\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 341 ---\n",
      "Prediction: Extreme poverty, or absolute poverty\n",
      "Label:      Extreme poverty, or absolute poverty\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 342 ---\n",
      "Prediction: Altruism or selflessness\n",
      "Label:      Altruism or selflessness\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 343 ---\n",
      "Prediction: August 31 is the 243r\n",
      "Label:      August 31 is the 243r\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 344 ---\n",
      "Prediction: An Abscess (Laca de los empeos) An Abscess\n",
      "Label:      An abscess (La\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.1308, Recall: 0.1750, F1: 0.0190\n",
      "--- Index 345 ---\n",
      "Prediction: (NandaCh\n",
      "Label:      nanda (Ch\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3496, Recall: 0.2954, F1: 0.3235\n",
      "--- Index 346 ---\n",
      "Prediction: The Articles of Confederation\n",
      "Label:      The Articles of Confederation\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 347 ---\n",
      "Prediction: Abner Doubleday (June\n",
      "Label:      Abner Doubleday (June\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 348 ---\n",
      "Prediction: Terms \"anno domini\" (\n",
      "Label:      The terms anno Domini (\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3519, Recall: 0.4018, F1: 0.3777\n",
      "--- Index 349 ---\n",
      "Prediction: Corresponding to most kinds of\n",
      "Label:      Corresponding to most kinds of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 350 ---\n",
      "Prediction: This is a list of characters\n",
      "Label:      This is a list of characters\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 351 ---\n",
      "Prediction: Alaric II (Goth\n",
      "Label:      Alaric II (Goth\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 352 ---\n",
      "Prediction: An acid An acid (from acid, the Latin\n",
      "Label:      An acid (from the Latin acid\n",
      "Match: No\n",
      "BLEU Score: 0.3247\n",
      "BERTScore - Precision: 0.5339, Recall: 0.7933, F1: 0.6612\n",
      "--- Index 353 ---\n",
      "Prediction: Achill Island (\n",
      "Label:      Achill Island (/k\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.7847, Recall: 0.5482, F1: 0.6645\n",
      "--- Index 354 ---\n",
      "Prediction: Alexander II was a king\n",
      "Label:      Alexander II was a king\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 355 ---\n",
      "Prediction: Anaximander (/\n",
      "Label:      Anaximander (/\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 356 ---\n",
      "Prediction: In the Hebrew Bible and the Qu\n",
      "Label:      In the Hebrew Bible and the Qu\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 357 ---\n",
      "Prediction: Ancylopoda is\n",
      "Label:      Ancylopoda is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 358 ---\n",
      "Prediction: An astronomer is a\n",
      "Label:      An astronomer is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 359 ---\n",
      "Prediction: The Austrian School is a\n",
      "Label:      The Austrian School is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 360 ---\n",
      "Prediction: An assembly line is a manufacturing\n",
      "Label:      An assembly line is a manufacturing\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 361 ---\n",
      "Prediction: The ancient Aramaic alphabet is\n",
      "Label:      The ancient Aramaic alphabet is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 362 ---\n",
      "Prediction: The meaning of the word American in\n",
      "Label:      The meaning of the word American in\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 363 ---\n",
      "Prediction: Alyattes, king\n",
      "Label:      Alyattes, king\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 364 ---\n",
      "Prediction: The Plague (French\n",
      "Label:      The Plague (French\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 365 ---\n",
      "Prediction: Animal Farm is an allegorical\n",
      "Label:      Animal Farm is an allegorical\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 366 ---\n",
      "Prediction: In Greek mythology, Alc\n",
      "Label:      In Greek mythology, Alc\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 367 ---\n",
      "Prediction: In mathematics and computer science, an\n",
      "Label:      In mathematics and computer science, an\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 368 ---\n",
      "Prediction: Anah (Arabic or Ana\n",
      "Label:      Anah or Ana (Arab\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6333, Recall: 0.7472, F1: 0.6902\n",
      "--- Index 369 ---\n",
      "Prediction: Albert Sidney Johnston (\n",
      "Label:      Albert Sidney Johnston (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 370 ---\n",
      "Prediction: Aarhus r or\n",
      "Label:      Aarhus or r\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.7737, Recall: 0.7737, F1: 0.7741\n",
      "--- Index 371 ---\n",
      "Prediction: Analysis of variance (ANOVA)\n",
      "Label:      Analysis of variance (ANOVA)\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 372 ---\n",
      "Prediction: April 26 is the\n",
      "Label:      April 26 is the 116th\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.7853, Recall: 0.5199, F1: 0.6500\n",
      "--- Index 373 ---\n",
      "Prediction: Mages abech Mages abech Maria of Andries,\n",
      "Label:      Agnes Maria of Andechs\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0810, Recall: 0.1187, F1: 0.1013\n",
      "--- Index 374 ---\n",
      "Prediction: Politics of Armenia takes place in\n",
      "Label:      Politics of Armenia takes place in\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 375 ---\n",
      "Prediction: Alexander Emmanuel Rodolphe Ag\n",
      "Label:      Alexander Emmanuel Rodolphe Ag\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 376 ---\n",
      "Prediction: The Dasyprocti\n",
      "Label:      The Dasyprocti\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 377 ---\n",
      "Prediction: Abortion is the ending of\n",
      "Label:      Abortion is the ending of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 378 ---\n",
      "Prediction: The Museum of Work, or Ar\n",
      "Label:      The Museum of Work, or Ar\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 379 ---\n",
      "Prediction: Anjax (Ac\n",
      "Label:      Ajax (Anc\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.1323, Recall: 0.3287, F1: 0.0891\n",
      "--- Index 380 ---\n",
      "Prediction: \"America the Beautiful\" is an\n",
      "Label:      \"America the Beautiful\" is an\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 381 ---\n",
      "Prediction: Alexander Graham Bell (March 3, 18\n",
      "Label:      Alexander Graham Bell (March 3, 18\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 382 ---\n",
      "Prediction: Antigua and The Barbud\n",
      "Label:      The Antigua and Barbud\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.3721, Recall: 0.4054, F1: 0.3897\n",
      "--- Index 383 ---\n",
      "Prediction: Alder is the common name of\n",
      "Label:      Alder is the common name of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 384 ---\n",
      "Prediction: Auto racing (also known as car\n",
      "Label:      Auto racing (also known as car\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 385 ---\n",
      "Prediction: Afroasiatic (Afro\n",
      "Label:      Afroasiatic (Afro\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 386 ---\n",
      "Prediction: In United States appellate procedure,\n",
      "Label:      In United States appellate procedure,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 387 ---\n",
      "Prediction: Afghanistan (/fn\n",
      "Label:      Afghanistan/fn\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0180, Recall: 0.3060, F1: 0.1593\n",
      "--- Index 388 ---\n",
      "Prediction: Asteroids are minor planets\n",
      "Label:      Asteroids are minor planets\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 389 ---\n",
      "Prediction: Alaska (\n",
      "Label:      Alaska (/ls\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5649, Recall: 0.3117, F1: 0.4362\n",
      "--- Index 390 ---\n",
      "Prediction: Algorithms for\n",
      "Label:      Algorithms for\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 391 ---\n",
      "Prediction: The Alps (\n",
      "Label:      The Alps (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8222, Recall: 0.8222, F1: 0.8225\n",
      "--- Index 392 ---\n",
      "Prediction: August 15 is the 227th\n",
      "Label:      August 15 is the 227th\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 393 ---\n",
      "Prediction: The aspect ratio of a geometric\n",
      "Label:      The aspect ratio of a geometric\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 394 ---\n",
      "Prediction: Alfred William Lawson (March 24,\n",
      "Label:      Alfred William Lawson (March 24,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 395 ---\n",
      "Prediction: The term abdominal surgery broadly covers surgical\n",
      "Label:      The term abdominal surgery broadly covers surgical\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 396 ---\n",
      "Prediction: Animation is the process of creating the\n",
      "Label:      Animation is the process of creating the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 397 ---\n",
      "Prediction: Antisemitism (\n",
      "Label:      Antisemitism (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 398 ---\n",
      "Prediction: Antigua and Barbuda\n",
      "Label:      Antigua and Barbuda\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 399 ---\n",
      "Prediction: Austrian German (German:\n",
      "Label:      Austrian German (German:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 400 ---\n",
      "Prediction: Amphibians are e\n",
      "Label:      Amphibians are e\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 401 ---\n",
      "Prediction: Alternate history or alternative history\n",
      "Label:      Alternate history or alternative history\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 402 ---\n",
      "Prediction: \"Amazing Grace\" is\n",
      "Label:      \"Amazing Grace\" is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 403 ---\n",
      "Prediction: An anagram is a\n",
      "Label:      An anagram is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 404 ---\n",
      "Prediction: Anbar ( ) was\n",
      "Label:      Anbar () was\n",
      "Match: No\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 0.4691, Recall: 0.5865, F1: 0.5279\n",
      "--- Index 405 ---\n",
      "Prediction: In planar geometry, an angle\n",
      "Label:      In planar geometry, an angle\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 406 ---\n",
      "Prediction: Alabaster is a name\n",
      "Label:      Alabaster is a name\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 407 ---\n",
      "Prediction: Ambiguity is a type\n",
      "Label:      Ambiguity is a type\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 408 ---\n",
      "Prediction: The Angolan Armed Force\n",
      "Label:      The Angolan Armed Force\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 409 ---\n",
      "Prediction: Agapanthus african\n",
      "Label:      Agapanthus african\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 410 ---\n",
      "Prediction: Alexander Mackenzie, PC (\n",
      "Label:      Alexander Mackenzie, PC (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 411 ---\n",
      "Prediction: Anatomy is the branch of\n",
      "Label:      Anatomy is the branch of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 412 ---\n",
      "Prediction: Saint Angilbert (c\n",
      "Label:      Saint Angilbert (c\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 413 ---\n",
      "Prediction: \"American shot\" is a\n",
      "Label:      \"American shot\" is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 414 ---\n",
      "Prediction: Anglicanism is\n",
      "Label:      Anglicanism is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 415 ---\n",
      "Prediction: Amati is the last name of\n",
      "Label:      Amati is the last name of\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 416 ---\n",
      "Prediction: AOL Inc. (previous\n",
      "Label:      AOL Inc. (previous\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 417 ---\n",
      "Prediction: Arraignment is\n",
      "Label:      Arraignment is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 418 ---\n",
      "Prediction: Anime (Hawaii) (Japanes\n",
      "Label:      Anime (Japanes\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.5002, Recall: 0.7872, F1: 0.6406\n",
      "--- Index 419 ---\n",
      "Prediction: Anazarbus (med. A\n",
      "Label:      Anazarbus (med. A\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 420 ---\n",
      "Prediction: anius Pi (Latina) anius Pi (\n",
      "Label:      Antoninus Pius (La\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: -0.2236, Recall: -0.0661, F1: -0.1443\n",
      "--- Index 421 ---\n",
      "Prediction: Aldine Press was the printing\n",
      "Label:      Aldine Press was the printing\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 422 ---\n",
      "Prediction: Alypius of Anti\n",
      "Label:      Alypius of Anti\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 423 ---\n",
      "Prediction: The American National Standards Institute (ANS\n",
      "Label:      The American National Standards Institute (ANS\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 424 ---\n",
      "Prediction: The alkali metals are\n",
      "Label:      The alkali metals are\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 425 ---\n",
      "Prediction: Alchemy is an influential tradition\n",
      "Label:      Alchemy is an influential tradition\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 426 ---\n",
      "Prediction: Analysis is the process of\n",
      "Label:      Analysis is the process of breaking\n",
      "Match: No\n",
      "BLEU Score: 0.8187\n",
      "BERTScore - Precision: 0.7058, Recall: 0.5829, F1: 0.6442\n",
      "--- Index 427 ---\n",
      "Prediction: Aleister Crowley (\n",
      "Label:      Aleister Crowley (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8932, Recall: 0.8932, F1: 0.8933\n",
      "--- Index 428 ---\n",
      "Prediction: Apocrypha are works,\n",
      "Label:      Apocrypha are works,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 429 ---\n",
      "Prediction: This article is about the demographic features\n",
      "Label:      This article is about the demographic features\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 430 ---\n",
      "Prediction: Amber is fossilized tree resin (\n",
      "Label:      Amber is fossilized tree resin (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 431 ---\n",
      "Prediction: Aldous Leonard Huxley\n",
      "Label:      Aldous Leonard Huxley\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 432 ---\n",
      "Prediction: Amplitude modulation (AM\n",
      "Label:      Amplitude modulation (AM\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 433 ---\n",
      "Prediction: An annual plant is a plant\n",
      "Label:      An annual plant is a plant\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 434 ---\n",
      "Prediction: Alexis Carrel (F\n",
      "Label:      Alexis Carrel (F\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 435 ---\n",
      "Prediction: Irwin Allen Ginsberg\n",
      "Label:      Irwin Allen Ginsberg\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 436 ---\n",
      "Prediction: Since the adoption of a new\n",
      "Label:      Since the adoption of a new\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 437 ---\n",
      "Prediction: Ayn Rand (\n",
      "Label:      Ayn Rand (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8757, Recall: 0.8757, F1: 0.8759\n",
      "--- Index 438 ---\n",
      "Prediction: Transport in Angola comprises:\n",
      "Label:      Transport in Angola comprises:\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 439 ---\n",
      "Prediction: Abdul Alhazred is a\n",
      "Label:      Abdul Alhazred is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 440 ---\n",
      "Prediction: Andr-Marie Ampre (\n",
      "Label:      Andr-Marie Ampre (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 441 ---\n",
      "Prediction: In computing, an applet is\n",
      "Label:      In computing, an applet is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 442 ---\n",
      "Prediction: In Greek mythology, Agame\n",
      "Label:      In Greek mythology, Agame\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 443 ---\n",
      "Prediction: aka ethics The aka ethics examination is applied philosophy\n",
      "Label:      Applied ethics is the philosophical examination\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4249, Recall: 0.5359, F1: 0.4807\n",
      "--- Index 444 ---\n",
      "Prediction: Berthold Konrad Hermann Albert\n",
      "Label:      Berthold Konrad Hermann Albert\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 445 ---\n",
      "Prediction: An anchor is a device,\n",
      "Label:      An anchor is a device,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 446 ---\n",
      "Prediction: Anguilla (\n",
      "Label:      Anguilla (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8337, Recall: 0.8337, F1: 0.8340\n",
      "--- Index 447 ---\n",
      "Prediction: Abydos (A\n",
      "Label:      Abydos (A\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 448 ---\n",
      "Prediction: An appellate court, commonly called\n",
      "Label:      An appellate court, commonly called\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 449 ---\n",
      "Prediction: Araian Araian Araia is a landlocked country\n",
      "Label:      Armenia is a landlocked country\n",
      "Match: No\n",
      "BLEU Score: 0.4111\n",
      "BERTScore - Precision: 0.2245, Recall: 0.7032, F1: 0.4541\n",
      "--- Index 450 ---\n",
      "Prediction: Amsterdam (English:/m\n",
      "Label:      Amsterdam (English/m\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6862, Recall: 0.6862, F1: 0.6867\n",
      "--- Index 451 ---\n",
      "Prediction: An abugida/\n",
      "Label:      An abugida/\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 452 ---\n",
      "Prediction: The aardvark (\n",
      "Label:      The aardvark (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 453 ---\n",
      "Prediction: Andorra (\n",
      "Label:      Andorra (/\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.8390, Recall: 0.8390, F1: 0.8393\n",
      "--- Index 454 ---\n",
      "Prediction: Ahmed II (Ottoman\n",
      "Label:      Ahmed II (Ottoman\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 455 ---\n",
      "Prediction: Arachnophobia or\n",
      "Label:      Arachnophobia or\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 456 ---\n",
      "Prediction: The Alan Parsons Project are\n",
      "Label:      The Alan Parsons Project are\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 457 ---\n",
      "Prediction: Ahmed III (Ottoman\n",
      "Label:      Ahmed III (Ottoman\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 458 ---\n",
      "Prediction: ALGOL (short for\n",
      "Label:      ALGOL (short for\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 459 ---\n",
      "Prediction: America's National Game\n",
      "Label:      America's National Game is\n",
      "Match: No\n",
      "BLEU Score: 0.7788\n",
      "BERTScore - Precision: 0.6816, Recall: 0.5283, F1: 0.6045\n",
      "--- Index 460 ---\n",
      "Prediction: The Andaman Islands (\n",
      "Label:      The Andaman Islands (\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 461 ---\n",
      "Prediction: Anxiety is an emotion\n",
      "Label:      Anxiety is an emotion\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 462 ---\n",
      "Prediction: The American Film Institute (AFI\n",
      "Label:      The American Film Institute (AFI\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 463 ---\n",
      "Prediction: The aardwolf (Pro\n",
      "Label:      The aardwolf (Pro\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 464 ---\n",
      "Prediction: An amide (/m\n",
      "Label:      An amide (/m\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 465 ---\n",
      "Prediction: Albania ( ), lbe\n",
      "Label:      Albania (/lbe\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.1654, Recall: 0.2068, F1: 0.1873\n",
      "--- Index 466 ---\n",
      "Prediction: Abydos/\n",
      "Label:      Abydos/\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 467 ---\n",
      "Prediction: The Academy Awards are the oldest awards\n",
      "Label:      The Academy Awards are the oldest awards\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 468 ---\n",
      "Prediction: Alpha (uppercase:\n",
      "Label:      Alpha (uppercase ,\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.6926, Recall: 0.6886, F1: 0.6911\n",
      "--- Index 469 ---\n",
      "Prediction: In Norse religion, As\n",
      "Label:      In Norse religion, As\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 470 ---\n",
      "Prediction: An axiom or post\n",
      "Label:      An axiom or post\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 471 ---\n",
      "Prediction: Amateur astronomy is\n",
      "Label:      Amateur astronomy is\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 472 ---\n",
      "Prediction: An android is a robot or\n",
      "Label:      An android is a robot or\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 473 ---\n",
      "Prediction: An allegiance is a\n",
      "Label:      An allegiance is a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 474 ---\n",
      "Prediction: This article will go through a\n",
      "Label:      This article will go through a\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 475 ---\n",
      "Prediction: In the ancient Greek myths,\n",
      "Label:      In the ancient Greek myths,\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 476 ---\n",
      "Prediction: The geography of Antarctica is\n",
      "Label:      The geography of Antarctica is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 477 ---\n",
      "Prediction: Ahenobarbus was\n",
      "Label:      Ahenobarbus was\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 478 ---\n",
      "Prediction: Ammonius Hermia\n",
      "Label:      Ammonius Hermia\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 479 ---\n",
      "Prediction:  The Atanasoff\n",
      "Label:      The Atanasoff\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.2557, Recall: 0.1706, F1: 0.2141\n",
      "--- Index 480 ---\n",
      "Prediction: Hymn to Pro\n",
      "Label:      Hymn to Pro\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 481 ---\n",
      "Prediction: An antipope (Lat\n",
      "Label:      An antipope (Lat\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 482 ---\n",
      "Prediction: Actinopterygii\n",
      "Label:      Actinopterygii\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 483 ---\n",
      "Prediction: Alexander II (Mediaeval Ga\n",
      "Label:      Alexander II (Mediaeval Ga\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 484 ---\n",
      "Prediction: International Atomic Time (TAI\n",
      "Label:      International Atomic Time (TAI\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 485 ---\n",
      "Prediction: Al aemanni The Al aemanni, or the Al\n",
      "Label:      The Alemanni (also Al\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.0375, Recall: -0.0180, F1: 0.0112\n",
      "--- Index 486 ---\n",
      "Prediction: The Armed Forces of the\n",
      "Label:      The Armed Forces of the\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 487 ---\n",
      "Prediction: Argon is a chemical element\n",
      "Label:      Argon is a chemical element\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 488 ---\n",
      "Prediction: A motor neuron disease (M\n",
      "Label:      A motor neuron disease (M\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 489 ---\n",
      "Prediction: In chemistry, an alcohol is\n",
      "Label:      In chemistry, an alcohol is\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 490 ---\n",
      "Prediction: The Alabama River , in U\n",
      "Label:      The Alabama River, in the U\n",
      "Match: No\n",
      "BLEU Score: 0.6732\n",
      "BERTScore - Precision: 0.8010, Recall: 0.7966, F1: 0.7992\n",
      "--- Index 491 ---\n",
      "Prediction: An archipelago (\n",
      "Label:      An archipelago (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 492 ---\n",
      "Prediction: August 25th is The 237\n",
      "Label:      August 25 is the 237th\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4606, Recall: 0.4813, F1: 0.4718\n",
      "--- Index 493 ---\n",
      "Prediction: Throughout history, forms of art\n",
      "Label:      Throughout history, forms of art\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 494 ---\n",
      "Prediction: Ashoka Maurya (\n",
      "Label:      Ashoka Maurya (\n",
      "Match: Yes\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 495 ---\n",
      "Prediction: Aimoin (c.\n",
      "Label:      Aimoin (c.\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 496 ---\n",
      "Prediction: Astatine is a very rare\n",
      "Label:      Astatine is a very rare\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "--- Index 497 ---\n",
      "Prediction: Albert Einstein (a\n",
      "Label:      Albert Einstein (/a\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.7060, Recall: 0.7060, F1: 0.7065\n",
      "--- Index 498 ---\n",
      "Prediction: Angola/\n",
      "Label:      Angola/n\n",
      "Match: No\n",
      "BLEU Score: 0.0000\n",
      "BERTScore - Precision: 0.4223, Recall: 0.0978, F1: 0.2562\n",
      "--- Index 499 ---\n",
      "Prediction: Apollo 11 was the spaceflight\n",
      "Label:      Apollo 11 was the spaceflight\n",
      "Match: Yes\n",
      "BLEU Score: 1.0000\n",
      "BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Score Calculation ---\n",
      "Total number of comparisons: 500\n",
      "Number of identical pairs: 380\n",
      "Proportion of identical lines: 0.76\n",
      "Average BLEU Score: 0.6366\n",
      "Median BLEU Score: 1.0000\n",
      "BLEU Score Standard Deviation: 0.4732\n",
      "\n",
      "--- BERTScore Metrics ---\n",
      "Average BERTScore - Precision: 0.8641, Recall: 0.8623, F1: 0.8631\n",
      "Median BERTScore - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "BERTScore Standard Deviation - Precision: 0.2960, Recall: 0.2899, F1: 0.2899\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_and_compare(log_file_path):\n",
    "    \"\"\"\n",
    "    Extracts predicted and labeled lines from a log file, compares them,\n",
    "    prints the comparisons, calculates BLEU and BERTScore, and\n",
    "    provides descriptive average metrics.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "        - proportion_identical: The proportion (mean) of identical lines.\n",
    "        - average_bleu: The average BLEU score.\n",
    "        - bertscore_precision: Average BERTScore precision.\n",
    "        - bertscore_recall: Average BERTScore recall.\n",
    "        - bertscore_f1: Average BERTScore F1.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_line(line):\n",
    "        \"\"\"Removes 'Index X:' prefix and extra spacing from a line.\"\"\"\n",
    "        line = re.sub(r\"^Index \\d+: \", \"\", line)\n",
    "        line = line.strip()\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "        line = re.sub(r\"\\s*/\\s*\", \"/\", line)\n",
    "        return line\n",
    "\n",
    "    preds = {}\n",
    "    labels = {}\n",
    "\n",
    "    with open(log_file_path, 'r') as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    preds_section = False\n",
    "    labels_section = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Contents of decoded_preds:\" in line:\n",
    "            preds_section = True\n",
    "            labels_section = False\n",
    "            continue\n",
    "        elif \"Contents of decoded_labels:\" in line:\n",
    "            preds_section = False\n",
    "            labels_section = True\n",
    "            continue\n",
    "\n",
    "        if preds_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                preds[index] = match.group(2)\n",
    "        elif labels_section:\n",
    "            match = re.match(r\"^Index (\\d+): (.*)\", line)\n",
    "            if match:\n",
    "                index = int(match.group(1))\n",
    "                labels[index] = match.group(2)\n",
    "\n",
    "    if len(preds) != len(labels):\n",
    "        print(f\"Warning: Number of predictions ({len(preds)}) and labels ({len(labels)}) differ.\")\n",
    "\n",
    "    min_len = min(len(preds), len(labels))\n",
    "\n",
    "    # Initialize BERTScorer\n",
    "    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "    identical_count = 0\n",
    "    bleu_scores = []\n",
    "    bertscore_precisions = []\n",
    "    bertscore_recalls = []\n",
    "    bertscore_f1s = []\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if i in preds and i in labels:\n",
    "            cleaned_pred = clean_line(preds[i])\n",
    "            cleaned_label = clean_line(labels[i])\n",
    "            print(f\"--- Index {i} ---\")\n",
    "            print(f\"Prediction: {cleaned_pred}\")\n",
    "            print(f\"Label:      {cleaned_label}\")\n",
    "\n",
    "            if cleaned_pred == cleaned_label:\n",
    "                identical_count += 1\n",
    "                print(\"Match: Yes\")\n",
    "            else:\n",
    "                print(\"Match: No\")\n",
    "\n",
    "            # BLEU Score\n",
    "            # Tokenize for BLEU (simple word tokenization)\n",
    "            pred_tokens = nltk.word_tokenize(cleaned_pred.lower())\n",
    "            label_tokens = nltk.word_tokenize(cleaned_label.lower())\n",
    "\n",
    "            bleu = nltk.translate.bleu_score.sentence_bleu([label_tokens], pred_tokens)\n",
    "            bleu_scores.append(bleu)\n",
    "\n",
    "            # BERTScore\n",
    "            P, R, F1 = scorer.score([cleaned_pred], [cleaned_label])\n",
    "            bertscore_precisions.append(P.item())\n",
    "            bertscore_recalls.append(R.item())\n",
    "            bertscore_f1s.append(F1.item())\n",
    "\n",
    "            print(f\"BLEU Score: {bleu:.4f}\")\n",
    "            print(f\"BERTScore - Precision: {P.item():.4f}, Recall: {R.item():.4f}, F1: {F1.item():.4f}\")\n",
    "\n",
    "    print(\"\\n--- Score Calculation ---\")\n",
    "    print(f\"Total number of comparisons: {min_len}\")\n",
    "    print(f\"Number of identical pairs: {identical_count}\")\n",
    "\n",
    "    proportion_identical = identical_count / min_len if min_len > 0 else 0.0\n",
    "    average_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "    print(f\"Proportion of identical lines: {proportion_identical}\")\n",
    "    print(f\"Average BLEU Score: {average_bleu:.4f}\")\n",
    "    print(f\"Median BLEU Score: {np.median(bleu_scores):.4f}\")\n",
    "    print(f\"BLEU Score Standard Deviation: {np.std(bleu_scores):.4f}\")\n",
    "\n",
    "    print(\"\\n--- BERTScore Metrics ---\")\n",
    "    print(f\"Average BERTScore - Precision: {np.mean(bertscore_precisions):.4f}, Recall: {np.mean(bertscore_recalls):.4f}, F1: {np.mean(bertscore_f1s):.4f}\")\n",
    "    print(f\"Median BERTScore - Precision: {np.median(bertscore_precisions):.4f}, Recall: {np.median(bertscore_recalls):.4f}, F1: {np.median(bertscore_f1s):.4f}\")\n",
    "    print(f\"BERTScore Standard Deviation - Precision: {np.std(bertscore_precisions):.4f}, Recall: {np.std(bertscore_recalls):.4f}, F1: {np.std(bertscore_f1s):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"proportion_identical\": proportion_identical,\n",
    "        \"average_bleu\": average_bleu,\n",
    "        \"bertscore_precision\": np.mean(bertscore_precisions),\n",
    "        \"bertscore_recall\": np.mean(bertscore_recalls),\n",
    "        \"bertscore_f1\": np.mean(bertscore_f1s)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    log_file_path = \"/home/jesse-wonnink/vec2text/scripts/outputs/tokenlengthsearch/repro_T2_gtr-50steps-4beam_dbpedia-entity_500samples_8maxtoken.log\"  # Replace with your actual path\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    try:\n",
    "        extract_and_compare(log_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"The specified log file was not found. Please make sure to provide a valid file path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLjesse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
