+++ Arguments Passed +++
Namespace(model='jxm/gtr__nq__32__correct', beir_dataset='msmarco', beam_width=8, steps=50, batch_size=16, max_samples=15, output_csv=None, max_length=52)
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=False, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=jxm/gtr__nq__32,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=6250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=gtr_corrector,
exp_name=,
experiment=corrector,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.00282842712,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct/runs/Nov05_04-29-22_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=50,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=200.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32__correct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=256,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=25000,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32__correct
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=True, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=1250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=oct-gtr32,
exp_name=,
experiment=inversion,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32/runs/Nov01_18-00-43_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=25,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant_with_warmup,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=300.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=512,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=125,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=625,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32
Loading datasets with TOKENIZERS_PARALLELISM = False
loading train dataset from path: /home/scur0989/.cache/inversion/dd0d97ad14fd6897b0d31cecc2e14d13.arrow
loaded dict of val datasets from /home/scur0989/.cache/inversion/8a11157c2dba245e22bfdea7946e149e.arrow
Loading BEIR dataset: msmarco
Original dataset size: 100000
Randomly selected 15 samples from the dataset.
Model tokens max_length: 52
Embedder tokens max_length: 52
+++ Trainer Args Passed +++
trainer.num_gen_recursive_steps: 50
trainer.sequence_beam_width: 8
Model name: jxm/gtr__nq__32__correct
max_seq_length: 512
Dataset({
    features: ['text'],
    num_rows: 15
})

=== Prediction-Label Pairs ===

Pair #1
--------------------------------------------------
[pred] in your Xbox.exe file and either download the pseud of Flash Blizzard or put the pseud
[true] 1 Download your gamer profile form your xbox or ps3 onto your usb flash drive. 2 Open the .exe file in the download and go to file. 3 Click on Open usbâ and find ur flash
--------------------------------------------------

Pair #2
--------------------------------------------------
[pred] House of Representatives vote today to pass the Painfully Unborn Child Protection Capability Bill, which calls for a bipartisan ban on abortions.
[true] Today, the House of Representatives votes on the Pain Capable Unborn Child Protection Act, a bill to ban abortions after 20 weeks and the expectation is the bill will pass with a strong bipartisan majority.
--------------------------------------------------

Pair #3
--------------------------------------------------
[pred] It's around 3.5 miles so it's the perfect path for me when I live at home. I'll run around twice a mile
[true] It's the perfect path for me as it is around 1.5 miles around and I live about a mile away, so if I run around it once and go home it's about a 3.5 mile run, twice and its a
--------------------------------------------------

Pair #4
--------------------------------------------------
[pred] Italy can be a relatively large country, but the weather throughout the boot tends to vary from year to year. This is not to mention the top
[true] Italy is not a huge country, but the weather from the top to the toe of this boot will vary pretty dramatically throughout the year. Plus, with the long coastlines and mountain ranges, the temperature can change in a matter of minutes
--------------------------------------------------

Pair #5
--------------------------------------------------
[pred] gold (black and back) are my favorite colors but now I love gold because gold is worth a lot of money and you can buy everything that you
[true] Gold and black are my favorite colors but, back to gold. Gold is awesome because gold is worth a lot of money and I love, love, love, love, and love money! You can buy what ever you want and I think that gold
--------------------------------------------------

Pair #6
--------------------------------------------------
[pred] Shoulder pain can come from a variety of sources. Two main causes for shoulder pain include: repetitive injury of muscles (e.g. painting on
[true] Shoulder pain can come from a number of sources. The two main causes of shoulder pain include: Injury to muscles and/or tendons from prolonged overuse â for example, performing a repetitive activity like painting for too long. Mus
--------------------------------------------------

Pair #7
--------------------------------------------------
[pred] pils. They dilate to control how much light enters the eye. Eye doctors may also dilate when the eye has become smaller or more
[true] They mainly dilate to control the amount of light that enters the eye. Pupils become smaller when the light is bright, and larger in the dark. Eye doctors dilate eyes to check for signs of disease. Eyes can also di
--------------------------------------------------

Pair #8
--------------------------------------------------
[pred] what sporophyte means (a plant that produces spores in its diploid phase). Video game (meiosis)
[true] Video shows what sporophyte means. A plant (or the diploid phase in its life cycle) which produces spores by meiosis in order to produce gametophytes.. Sporo...
--------------------------------------------------

Pair #9
--------------------------------------------------
[pred] Some symptoms of a UTI include feeling "full" of the rectum, side discomfort, and back pain (rare in men). Blood
[true] Symptoms of a UTI include general discomfort, a feeling of being over-tired, blood in the urine and pain even when not urinating. Men might feel fullness in the rectum. Back and side pain
--------------------------------------------------

Pair #10
--------------------------------------------------
[pred] The peak half-life for hydrocodone levels in Lortab is about 60 hours. The peak half life for hydrocodone for adults reached
[true] Peak levels and half life of Lortab. The half life for the hydrocodone in Lortab in the typical adult is just under 4 hours. Hydrocodone reaches its peak level in the blood after about 1.3 hours. However,
--------------------------------------------------


{'eval_loss': 10.188558578491211, 'eval_model_preparation_time': 0.0185, 'eval_pred_num_tokens': 31.000001907348633, 'eval_true_num_tokens': 51.466670989990234, 'eval_token_set_precision': 0.47811445526684676, 'eval_token_set_recall': 0.6886048508856424, 'eval_token_set_f1': 0.5611582393466894, 'eval_token_set_f1_sem': np.float64(0.038925635285903454), 'eval_n_ngrams_match_1': 16.666666666666668, 'eval_n_ngrams_match_2': 6.4, 'eval_n_ngrams_match_3': 3.1333333333333333, 'eval_num_true_words': 40.46666666666667, 'eval_num_pred_words': 24.866666666666667, 'eval_bleu_score': np.float64(10.185068212251991), 'eval_bleu_score_sem': np.float64(1.6956185191398667), 'eval_rouge_score': np.float64(0.5591270373571696), 'eval_exact_match': np.float64(0.0), 'eval_exact_match_sem': np.float64(0.0), 'eval_emb_cos_sim': 0.8903867602348328, 'eval_emb_cos_sim_sem': np.float64(0.020497091281499246), 'eval_emb_top1_equal': 0.6000000238418579, 'eval_emb_top1_equal_sem': np.float64(0.13093074505393074), 'eval_runtime': 373.9328, 'eval_samples_per_second': 0.04, 'eval_steps_per_second': 0.003}
+++ Evaluation Metrics +++
eval_loss: 10.188558578491211
eval_model_preparation_time: 0.0185
eval_pred_num_tokens: 31.000001907348633
eval_true_num_tokens: 51.466670989990234
eval_token_set_precision: 0.47811445526684676
eval_token_set_recall: 0.6886048508856424
eval_token_set_f1: 0.5611582393466894
eval_token_set_f1_sem: 0.038925635285903454
eval_n_ngrams_match_1: 16.666666666666668
eval_n_ngrams_match_2: 6.4
eval_n_ngrams_match_3: 3.1333333333333333
eval_num_true_words: 40.46666666666667
eval_num_pred_words: 24.866666666666667
eval_bleu_score: 10.185068212251991
eval_bleu_score_sem: 1.6956185191398667
eval_rouge_score: 0.5591270373571696
eval_exact_match: 0.0
eval_exact_match_sem: 0.0
eval_emb_cos_sim: 0.8903867602348328
eval_emb_cos_sim_sem: 0.020497091281499246
eval_emb_top1_equal: 0.6000000238418579
eval_emb_top1_equal_sem: 0.13093074505393074
eval_runtime: 373.9328
eval_samples_per_second: 0.04
eval_steps_per_second: 0.003
Time taken: 373.96515107154846
Current memory usage: 4.71MB; Peak: 10.93MB
