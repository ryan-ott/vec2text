+++ Arguments Passed +++
Namespace(model='jxm/gtr__nq__32__correct', beir_dataset='msmarco', beam_width=8, steps=50, batch_size=16, max_samples=15, output_csv=None, max_length=56)
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=False, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=jxm/gtr__nq__32,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=6250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=gtr_corrector,
exp_name=,
experiment=corrector,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.00282842712,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct/runs/Nov05_04-29-22_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=50,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=200.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32__correct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=256,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=25000,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32__correct
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=True, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=1250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=oct-gtr32,
exp_name=,
experiment=inversion,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32/runs/Nov01_18-00-43_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=25,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant_with_warmup,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=300.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=512,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=125,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=625,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32
Loading datasets with TOKENIZERS_PARALLELISM = False
loading train dataset from path: /home/scur0989/.cache/inversion/dd0d97ad14fd6897b0d31cecc2e14d13.arrow
loadedÂ dict of val datasets from /home/scur0989/.cache/inversion/8a11157c2dba245e22bfdea7946e149e.arrow
Loading BEIR dataset: msmarco
Original dataset size: 100000
Randomly selected 15 samples from the dataset.
Model tokens max_length: 56
Embedder tokens max_length: 56
+++ Trainer Args Passed +++
trainer.num_gen_recursive_steps: 50
trainer.sequence_beam_width: 8
Model name: jxm/gtr__nq__32__correct
max_seq_length: 512
Dataset({
    features: ['text'],
    num_rows: 15
})

=== Prediction-Label Pairs ===

Pair #1
--------------------------------------------------
[pred] on your PS3 flash drive and put in the .exe file of your Xbox Gamer or simply download the .exe file of your Ub
[true] 1 Download your gamer profile form your xbox or ps3 onto your usb flash drive. 2 Open the .exe file in the download and go to file. 3 Click on Open usbÃ¢ and find ur flash drive. 4 It
--------------------------------------------------

Pair #2
--------------------------------------------------
[pred] House of Representatives vote today to pass the Painfully Unborn Child Protection Capability Bill, which calls for a bipartisan ban on abortions.
[true] Today, the House of Representatives votes on the Pain Capable Unborn Child Protection Act, a bill to ban abortions after 20 weeks and the expectation is the bill will pass with a strong bipartisan majority.
--------------------------------------------------

Pair #3
--------------------------------------------------
[pred] it seems like a perfect path so when I run around it it's 2.5 miles so when I'm home it's 5 miles again.
[true] It's the perfect path for me as it is around 1.5 miles around and I live about a mile away, so if I run around it once and go home it's about a 3.5 mile run, twice and its a 5 mile run.
--------------------------------------------------

Pair #4
--------------------------------------------------
[pred] Italy can be a relatively large country, but the weather throughout the year of the boot tends to change from coast to coast, depending on the top
[true] Italy is not a huge country, but the weather from the top to the toe of this boot will vary pretty dramatically throughout the year. Plus, with the long coastlines and mountain ranges, the temperature can change in a matter of minutes as you go from
--------------------------------------------------

Pair #5
--------------------------------------------------
[pred] gold and black are my favorite colors back and forth, but I love gold because gold is what you buy now and it's worth all the money you
[true] Gold and black are my favorite colors but, back to gold. Gold is awesome because gold is worth a lot of money and I love, love, love, love, and love money! You can buy what ever you want and I think that gold and black are the
--------------------------------------------------

Pair #6
--------------------------------------------------
[pred] Muscle pain can come from a variety of sources. Two main causes for shoulder pain include: prolonged injury of muscles (e.g.: painting
[true] Shoulder pain can come from a number of sources. The two main causes of shoulder pain include: Injury to muscles and/or tendons from prolonged overuse Ã¢ for example, performing a repetitive activity like painting for too long. Muscles and/
--------------------------------------------------

Pair #7
--------------------------------------------------
[pred] they dilate to control how light enters the eye. Pupils tend to be smaller when the amount of light becomes brighter. Doctors usually
[true] They mainly dilate to control the amount of light that enters the eye. Pupils become smaller when the light is bright, and larger in the dark. Eye doctors dilate eyes to check for signs of disease. Eyes can also dilate as a
--------------------------------------------------

Pair #8
--------------------------------------------------
[pred] what sporophyte means (a plant that produces spores in its diploid phase). Video game (meiosis)
[true] Video shows what sporophyte means. A plant (or the diploid phase in its life cycle) which produces spores by meiosis in order to produce gametophytes.. Sporo...
--------------------------------------------------

Pair #9
--------------------------------------------------
[pred] As with other men, UTI symptoms include discomfort and feeling 'full' of the body. This can include back pain, side cramps,
[true] Symptoms of a UTI include general discomfort, a feeling of being over-tired, blood in the urine and pain even when not urinating. Men might feel fullness in the rectum. Back and side pain can indicate the infection
--------------------------------------------------

Pair #10
--------------------------------------------------
[pred] The peak half life for Lortab at hydrocodone levels is around 4 hours. The peak half life for Lortab has been concluded at the adult
[true] Peak levels and half life of Lortab. The half life for the hydrocodone in Lortab in the typical adult is just under 4 hours. Hydrocodone reaches its peak level in the blood after about 1.3 hours. However, the amount of Lor
--------------------------------------------------


{'eval_loss': 9.965882301330566, 'eval_model_preparation_time': 0.0183, 'eval_pred_num_tokens': 31.000001907348633, 'eval_true_num_tokens': 54.93333435058594, 'eval_token_set_precision': 0.4422537243125479, 'eval_token_set_recall': 0.6792766063573035, 'eval_token_set_f1': 0.5338489058320903, 'eval_token_set_f1_sem': np.float64(0.02530295121632589), 'eval_n_ngrams_match_1': 17.2, 'eval_n_ngrams_match_2': 6.2, 'eval_n_ngrams_match_3': 3.066666666666667, 'eval_num_true_words': 43.266666666666666, 'eval_num_pred_words': 25.333333333333332, 'eval_bleu_score': np.float64(9.004388720968896), 'eval_bleu_score_sem': np.float64(1.278069913957731), 'eval_rouge_score': np.float64(0.5486034479736844), 'eval_exact_match': np.float64(0.0), 'eval_exact_match_sem': np.float64(0.0), 'eval_emb_cos_sim': 0.905674397945404, 'eval_emb_cos_sim_sem': np.float64(0.01312352794671681), 'eval_emb_top1_equal': 0.3333333432674408, 'eval_emb_top1_equal_sem': np.float64(0.1259881629155793), 'eval_runtime': 374.2182, 'eval_samples_per_second': 0.04, 'eval_steps_per_second': 0.003}
+++ Evaluation Metrics +++
eval_loss: 9.965882301330566
eval_model_preparation_time: 0.0183
eval_pred_num_tokens: 31.000001907348633
eval_true_num_tokens: 54.93333435058594
eval_token_set_precision: 0.4422537243125479
eval_token_set_recall: 0.6792766063573035
eval_token_set_f1: 0.5338489058320903
eval_token_set_f1_sem: 0.02530295121632589
eval_n_ngrams_match_1: 17.2
eval_n_ngrams_match_2: 6.2
eval_n_ngrams_match_3: 3.066666666666667
eval_num_true_words: 43.266666666666666
eval_num_pred_words: 25.333333333333332
eval_bleu_score: 9.004388720968896
eval_bleu_score_sem: 1.278069913957731
eval_rouge_score: 0.5486034479736844
eval_exact_match: 0.0
eval_exact_match_sem: 0.0
eval_emb_cos_sim: 0.905674397945404
eval_emb_cos_sim_sem: 0.01312352794671681
eval_emb_top1_equal: 0.3333333432674408
eval_emb_top1_equal_sem: 0.1259881629155793
eval_runtime: 374.2182
eval_samples_per_second: 0.04
eval_steps_per_second: 0.003
Time taken: 374.25104451179504
Current memory usage: 2.67MB; Peak: 8.84MB
