/gpfs/home6/scur0989/v2t-env/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/gpfs/home6/scur0989/v2t-env/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Project Root: /gpfs/home6/scur0989/vec2text/reproduction
Outputs Directory: /gpfs/home6/scur0989/vec2text/reproduction/outputs
+++ Arguments Passed +++
Namespace(model='jxm/gtr__nq__32__correct', beir_dataset='webis-touche2020', beam_width=4, steps=50, batch_size=16, max_samples=10, output_csv=None, max_length=76, max_seq_length=512)
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=False, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=jxm/gtr__nq__32,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=6250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=gtr_corrector,
exp_name=,
experiment=corrector,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.00282842712,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct/runs/Nov05_04-29-22_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=50,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=200.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32__correct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=256,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=25000,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32__correct
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=True, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=1250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=oct-gtr32,
exp_name=,
experiment=inversion,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32/runs/Nov01_18-00-43_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=25,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant_with_warmup,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=300.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=512,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=125,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=625,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32
Loading datasets with TOKENIZERS_PARALLELISM = False
loading train dataset from path: /home/scur0989/.cache/inversion/dd0d97ad14fd6897b0d31cecc2e14d13.arrow
loadedÂ dict of val datasets from /home/scur0989/.cache/inversion/8a11157c2dba245e22bfdea7946e149e.arrow
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:01,  6.46it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  6.37it/s]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  6.24it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  6.14it/s]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  6.30it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  6.43it/s]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  6.37it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  6.76it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  6.48it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:00<00:00,  5.81it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:00<00:00,  5.49it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:00<00:00,  5.59it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:00<00:00,  5.72it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00,  5.99it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.54it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.10it/s]
Loading BEIR dataset: webis-touche2020
Original dataset size: 100000
cut dataset at 10 samples.
Tokenizing dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Tokenizing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 371.24 examples/s]
Model tokens max_length: 76
Embedder tokens max_length: 76
+++ Trainer Args Passed +++
trainer.num_gen_recursive_steps: 50
trainer.sequence_beam_width: 4
Model name: jxm/gtr__nq__32__correct
max_seq_length: 512
Dataset({
    features: ['text'],
    num_rows: 10
})
generating from val:   0%|          | 0/1 [00:00<?, ?it/s]generating from val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.71s/it]                                                                  
=== Prediction-Label Pairs ===
Length of decoded_preds: 10
Length of decoded_labels: 10

Contents of decoded_preds:
Index 0: do we invented all these amazing things - American polymers, bank bill scanners, WiFi satellites etc. But now I want to show the world
Index 1: reasons. Schools have no purpose to provide healthcare to those wishing to engage in contraception. It is, however, a matter of contention for many
Index 2: always Australia has been put into the shadow behind countries like America, England and Canada, but I feel Australia should not be put in the darkness. This debate
Index 3: be a country that Australia is to be in a position of being "significant": https://en.wikipedia.org/en/default/
Index 4: idea to "opt out". Funding your condom list will prove to be costly to the school and will be presented to the public at the beginning of
Index 5:              and then anything else.
Index 6: I accept.              
Index 7: you win. It is a good option for students to engage in arguments that have not been explored in the past. TUlate has been accused of
Index 8: been expressing a desire to have teenage high school districts focus on forming contraception for those who may or may not be able to do so
Index 9: why would God have called a christian murderer? You would not have been able to "forgive" me because it is difficult

Contents of decoded_labels:
Index 0: First of all we invented amazing things like WiFi, Google Maps, Polymer bank notes (if you are American and do not know what they are, they are plastic WATERPROOF bills), Ultrasound scanners, stainless steel braces and many more things. Why put us into the shadow if we have made such amazing things the whole world uses nowadays!
Index 1: Schools have no compelling interest in providing contraceptives to students. The purpose of schools is not to provide healthcare nor to provide any other service except insofar as it relates to the furtherance of education [1,2,3], though I do not contest that individual districts ought to have this option if they so choose. As an educator, I do feel that
Index 2: Australia has always been put into the shadow behind countries like America, Canada and even England sometimes, I feel Australia should not be put in the darkness and in this debate I will tell you why.
Index 3: The resolution used by Pro *assumes* that Australia isn't already a 'significant' country - however, in actual reality, it is. Firstly we should clarify what significance means: 1.a the state or quality of being significant1.b of consequence or importance==============
Index 4: How do you propose the school will fund your program? Condoms cost money and checking an "opt out" list before handing them out takes time away from staff members whenever they could be doing their actual jobs. Your "opt out" option is only be a token to parental authority and would be easily subverted. If everyone in school except a handful of
Index 5: Alright then.
Index 6: I accept.
Index 7: My opponent forfeited every round. None of my arguments were answered. I donâ€™t like the idea of winning by default, but here we are.Tule: itâ€™s good for students to get involved and address big issues like teen pregnancy. You need to be able to answer arguments like mine and not simply prepare for an abstinence
Index 8: As a senior at my school. My group and I are focusing on teenage pregnancy; we are determined to have high school districts provide contraceptive forms to students to be safe about sex. This focus isn't for us to encourage to have sex, but if teenagers decide to have sex to please be safe about it
Index 9: Why is it that so-called christians, Because there is no such a thing as a christian, Have serious trouble as READING and COMPREHENDING? Its not that difficult, Nor is it that hard. It was stated unto you a very simple "* "You are asking why God would forgive the murder

Pair #1
--------------------------------------------------
 index: 1
[pred] reasons. Schools have no purpose to provide healthcare to those wishing to engage in contraception. It is, however, a matter of contention for many
[true] Schools have no compelling interest in providing contraceptives to students. The purpose of schools is not to provide healthcare nor to provide any other service except insofar as it relates to the furtherance of education [1,2,3], though I do not contest that individual districts ought to have this option if they so choose. As an educator, I do feel that
--------------------------------------------------

Pair #2
--------------------------------------------------
 index: 0
[pred] do we invented all these amazing things - American polymers, bank bill scanners, WiFi satellites etc. But now I want to show the world
[true] First of all we invented amazing things like WiFi, Google Maps, Polymer bank notes (if you are American and do not know what they are, they are plastic WATERPROOF bills), Ultrasound scanners, stainless steel braces and many more things. Why put us into the shadow if we have made such amazing things the whole world uses nowadays!
--------------------------------------------------

Pair #3
--------------------------------------------------
 index: 4
[pred] idea to "opt out". Funding your condom list will prove to be costly to the school and will be presented to the public at the beginning of
[true] How do you propose the school will fund your program? Condoms cost money and checking an "opt out" list before handing them out takes time away from staff members whenever they could be doing their actual jobs. Your "opt out" option is only be a token to parental authority and would be easily subverted. If everyone in school except a handful of
--------------------------------------------------

Pair #4
--------------------------------------------------
 index: 9
[pred] why would God have called a christian murderer? You would not have been able to "forgive" me because it is difficult
[true] Why is it that so-called christians, Because there is no such a thing as a christian, Have serious trouble as READING and COMPREHENDING? Its not that difficult, Nor is it that hard. It was stated unto you a very simple "* "You are asking why God would forgive the murder
--------------------------------------------------

Pair #5
--------------------------------------------------
 index: 6
[pred] I accept.              
[true] I accept.
--------------------------------------------------

Pair #6
--------------------------------------------------
 index: 5
[pred]              and then anything else.
[true] Alright then.
--------------------------------------------------

Pair #7
--------------------------------------------------
 index: 8
[pred] been expressing a desire to have teenage high school districts focus on forming contraception for those who may or may not be able to do so
[true] As a senior at my school. My group and I are focusing on teenage pregnancy; we are determined to have high school districts provide contraceptive forms to students to be safe about sex. This focus isn't for us to encourage to have sex, but if teenagers decide to have sex to please be safe about it
--------------------------------------------------

Pair #8
--------------------------------------------------
 index: 2
[pred] always Australia has been put into the shadow behind countries like America, England and Canada, but I feel Australia should not be put in the darkness. This debate
[true] Australia has always been put into the shadow behind countries like America, Canada and even England sometimes, I feel Australia should not be put in the darkness and in this debate I will tell you why.
--------------------------------------------------


{'eval_loss': 12.450830459594727, 'eval_model_preparation_time': 0.0199, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 58.10000228881836, 'eval_token_set_precision': 0.4096092015718936, 'eval_token_set_recall': 0.5911737305509683, 'eval_token_set_f1': 0.4600846807545551, 'eval_token_set_f1_sem': np.float64(0.08104153226003724), 'eval_n_ngrams_match_1': 12.4, 'eval_n_ngrams_match_2': 4.1, 'eval_n_ngrams_match_3': 2.4, 'eval_num_true_words': 47.5, 'eval_num_pred_words': 22.2, 'eval_bleu_score': np.float64(7.6321641117815915), 'eval_bleu_score_sem': np.float64(4.743066866608192), 'eval_rouge_score': np.float64(0.428471912214475), 'eval_exact_match': np.float64(0.0), 'eval_exact_match_sem': np.float64(0.0), 'eval_emb_cos_sim': 0.7999688982963562, 'eval_emb_cos_sim_sem': np.float64(0.05198534143726401), 'eval_emb_top1_equal': 0.20000000298023224, 'eval_emb_top1_equal_sem': np.float64(0.13333333557853477), 'eval_runtime': 95.9912, 'eval_samples_per_second': 0.104, 'eval_steps_per_second': 0.01}
+++ Evaluation Metrics +++
eval_loss: 12.450830459594727
eval_model_preparation_time: 0.0199
eval_pred_num_tokens: 31.0
eval_true_num_tokens: 58.10000228881836
eval_token_set_precision: 0.4096092015718936
eval_token_set_recall: 0.5911737305509683
eval_token_set_f1: 0.4600846807545551
eval_token_set_f1_sem: 0.08104153226003724
eval_n_ngrams_match_1: 12.4
eval_n_ngrams_match_2: 4.1
eval_n_ngrams_match_3: 2.4
eval_num_true_words: 47.5
eval_num_pred_words: 22.2
eval_bleu_score: 7.6321641117815915
eval_bleu_score_sem: 4.743066866608192
eval_rouge_score: 0.428471912214475
eval_exact_match: 0.0
eval_exact_match_sem: 0.0
eval_emb_cos_sim: 0.7999688982963562
eval_emb_cos_sim_sem: 0.05198534143726401
eval_emb_top1_equal: 0.20000000298023224
eval_emb_top1_equal_sem: 0.13333333557853477
eval_runtime: 95.9912
eval_samples_per_second: 0.104
eval_steps_per_second: 0.01
Time taken: 96.02382516860962
Current memory usage: 3.39MB; Peak: 9.59MB
I1222 01:15:11.497680 2690224 torch/_dynamo/utils.py:399] TorchDynamo compilation metrics:
I1222 01:15:11.497680 2690224 torch/_dynamo/utils.py:399] Function    Runtimes (s)
I1222 01:15:11.497680 2690224 torch/_dynamo/utils.py:399] ----------  --------------
