/gpfs/home6/scur0989/v2t-env/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/gpfs/home6/scur0989/v2t-env/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Project Root: /gpfs/home6/scur0989/vec2text/reproduction
Outputs Directory: /gpfs/home6/scur0989/vec2text/reproduction/outputs
+++ Arguments Passed +++
Namespace(model='jxm/gtr__nq__32__correct', beir_dataset='webis-touche2020', beam_width=4, steps=50, batch_size=16, max_samples=10, output_csv=None, max_length=60, max_seq_length=512)
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=False, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=jxm/gtr__nq__32,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=6250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=gtr_corrector,
exp_name=,
experiment=corrector,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.00282842712,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct/runs/Nov05_04-29-22_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=50,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=200.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32__correct,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=256,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32-correct,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=25000,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32__correct
Set num workers to 1
+++ model_args +++
ModelArguments(model_name_or_path='t5-base', embedder_model_name='gtr_base', embedder_model_api=None, embedder_gaussian_noise_level=0.0, embedder_torch_dtype='float32', embedding_transform_strategy='repeat', encoder_dropout_disabled=False, decoder_dropout_disabled=False, model_type=None, config_overrides=None, config_name=None, tokenizer_name=None, cache_dir=None, model_revision='main', max_seq_length=32, torch_dtype=None, num_repeat_tokens=16, embedding_zero_except_topk=None, embedder_no_grad=True, use_lora=False, embedder_fake_with_zeros=False, use_frozen_embeddings_as_input=True, corrector_ignore_hypothesis_embedding=False, embeddings_from_layer_n=None, freeze_strategy='none')

+++ data_args +++
DataArguments(dataset_name='nq', max_eval_samples=500, use_less_data=1000)

+++ training_args +++
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-06,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=0,
bf16_full_eval=False,
cheat_on_train_hypotheses=False,
corrector_model_alias=None,
corrector_model_from_pretrained=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=1250,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
exp_group_name=oct-gtr32,
exp_name=,
experiment=inversion,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=True,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=['inputs'],
include_inputs_for_metrics=True,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.002,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wentingz/vec2text/saves/gtr-nq-msl32/runs/Nov01_18-00-43_mosaic-cirrascale-03.reviz.ai2.in,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=25,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant_with_warmup,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=nq_loss,
mock_embedder=False,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=True,
num_train_epochs=300.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=saves/jxm__gtr__nq__32,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=256,
per_device_train_batch_size=512,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/home/wentingz/vec2text/saves/gtr-nq-msl32,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=125,
save_strategy=steps,
save_total_limit=2,
seed=42,
skip_memory_metrics=True,
split_batches=None,
steps_per_epoch=500000,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
use_wandb=False,
warmup_ratio=0.0,
warmup_steps=625,
weight_decay=0.0,
)

Experiment output_dir = saves/jxm__gtr__nq__32
Loading datasets with TOKENIZERS_PARALLELISM = False
loading train dataset from path: /home/scur0989/.cache/inversion/dd0d97ad14fd6897b0d31cecc2e14d13.arrow
loadedÂ dict of val datasets from /home/scur0989/.cache/inversion/8a11157c2dba245e22bfdea7946e149e.arrow
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:01,  6.46it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  6.37it/s]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  6.28it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  6.06it/s]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  6.36it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  6.59it/s]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  6.51it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  6.89it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  6.57it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:00<00:00,  5.30it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:00<00:00,  5.81it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:00<00:00,  5.94it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:00<00:00,  5.96it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:00<00:00,  6.20it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.72it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.28it/s]
Loading BEIR dataset: webis-touche2020
Original dataset size: 100000
cut dataset at 10 samples.
Tokenizing dataset:   0%|          | 0/10 [00:00<?, ? examples/s]Tokenizing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 381.79 examples/s]
Model tokens max_length: 60
Embedder tokens max_length: 60
+++ Trainer Args Passed +++
trainer.num_gen_recursive_steps: 50
trainer.sequence_beam_width: 4
Model name: jxm/gtr__nq__32__correct
max_seq_length: 512
Dataset({
    features: ['text'],
    num_rows: 10
})
generating from val:   0%|          | 0/1 [00:00<?, ?it/s]generating from val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.50s/it]                                                                  
=== Prediction-Label Pairs ===
Length of decoded_preds: 10
Length of decoded_labels: 10

Contents of decoded_preds:
Index 0: are Americans, why did we invent these amazing things?Stainless steel plastic banknotes, WiFi, GEAR scanners, Google Maps and so
Index 1: The purpose for schools is not to provide contraceptives to healthcare. Whether or not schools have any compelling interest in continuing to do so remains a
Index 2: always Australia has been put into the shadow behind countries like America, England and Canada, but I feel Australia should not be put in the darkness. This debate
Index 3: to view Australia as a country which it would be welcome to rectify if the reality were to become established. 1. What significance means is a
Index 4: money costs before handing them out. Choosing to opt out from the condom list will prove to school and program staff that it is a good
Index 5:              and then anything else.
Index 6: I accept.              
Index 7: you win a round it is okay to engage in arguments if the student is guilty. tule has gained a lot of popularity in recent
Index 8: I have a suggestion that high school districts are focusing on providing contraceptive forms to teens. We are determined to ensure that we do not have
Index 9: why is there such a thing as so-called christians? It is not a TRIUDDY reading. I will go on to

Contents of decoded_labels:
Index 0: First of all we invented amazing things like WiFi, Google Maps, Polymer bank notes (if you are American and do not know what they are, they are plastic WATERPROOF bills), Ultrasound scanners, stainless steel braces and many more things. Why put us into
Index 1: Schools have no compelling interest in providing contraceptives to students. The purpose of schools is not to provide healthcare nor to provide any other service except insofar as it relates to the furtherance of education [1,2,3], though I do not contest that individual districts ought to have
Index 2: Australia has always been put into the shadow behind countries like America, Canada and even England sometimes, I feel Australia should not be put in the darkness and in this debate I will tell you why.
Index 3: The resolution used by Pro *assumes* that Australia isn't already a 'significant' country - however, in actual reality, it is. Firstly we should clarify what significance means: 1.a the state or quality of being significant1.b of consequence
Index 4: How do you propose the school will fund your program? Condoms cost money and checking an "opt out" list before handing them out takes time away from staff members whenever they could be doing their actual jobs. Your "opt out" option is only be a token to parental authority
Index 5: Alright then.
Index 6: I accept.
Index 7: My opponent forfeited every round. None of my arguments were answered. I donâ€™t like the idea of winning by default, but here we are.Tule: itâ€™s good for students to get involved and address big issues like teen pregnancy. You need to be 
Index 8: As a senior at my school. My group and I are focusing on teenage pregnancy; we are determined to have high school districts provide contraceptive forms to students to be safe about sex. This focus isn't for us to encourage to have sex, but
Index 9: Why is it that so-called christians, Because there is no such a thing as a christian, Have serious trouble as READING and COMPREHENDING? Its not that difficult, Nor is it that hard. It was stated unto you

Pair #1
--------------------------------------------------
 index: 1
[pred] The purpose for schools is not to provide contraceptives to healthcare. Whether or not schools have any compelling interest in continuing to do so remains a
[true] Schools have no compelling interest in providing contraceptives to students. The purpose of schools is not to provide healthcare nor to provide any other service except insofar as it relates to the furtherance of education [1,2,3], though I do not contest that individual districts ought to have
--------------------------------------------------

Pair #2
--------------------------------------------------
 index: 0
[pred] are Americans, why did we invent these amazing things?Stainless steel plastic banknotes, WiFi, GEAR scanners, Google Maps and so
[true] First of all we invented amazing things like WiFi, Google Maps, Polymer bank notes (if you are American and do not know what they are, they are plastic WATERPROOF bills), Ultrasound scanners, stainless steel braces and many more things. Why put us into
--------------------------------------------------

Pair #3
--------------------------------------------------
 index: 4
[pred] money costs before handing them out. Choosing to opt out from the condom list will prove to school and program staff that it is a good
[true] How do you propose the school will fund your program? Condoms cost money and checking an "opt out" list before handing them out takes time away from staff members whenever they could be doing their actual jobs. Your "opt out" option is only be a token to parental authority
--------------------------------------------------

Pair #4
--------------------------------------------------
 index: 9
[pred] why is there such a thing as so-called christians? It is not a TRIUDDY reading. I will go on to
[true] Why is it that so-called christians, Because there is no such a thing as a christian, Have serious trouble as READING and COMPREHENDING? Its not that difficult, Nor is it that hard. It was stated unto you
--------------------------------------------------

Pair #5
--------------------------------------------------
 index: 6
[pred] I accept.              
[true] I accept.
--------------------------------------------------

Pair #6
--------------------------------------------------
 index: 5
[pred]              and then anything else.
[true] Alright then.
--------------------------------------------------

Pair #7
--------------------------------------------------
 index: 8
[pred] I have a suggestion that high school districts are focusing on providing contraceptive forms to teens. We are determined to ensure that we do not have
[true] As a senior at my school. My group and I are focusing on teenage pregnancy; we are determined to have high school districts provide contraceptive forms to students to be safe about sex. This focus isn't for us to encourage to have sex, but
--------------------------------------------------

Pair #8
--------------------------------------------------
 index: 2
[pred] always Australia has been put into the shadow behind countries like America, England and Canada, but I feel Australia should not be put in the darkness. This debate
[true] Australia has always been put into the shadow behind countries like America, Canada and even England sometimes, I feel Australia should not be put in the darkness and in this debate I will tell you why.
--------------------------------------------------


{'eval_loss': 12.47735595703125, 'eval_model_preparation_time': 0.0196, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 46.900001525878906, 'eval_token_set_precision': 0.4611165272628586, 'eval_token_set_recall': 0.6260221051090615, 'eval_token_set_f1': 0.513638792395682, 'eval_token_set_f1_sem': np.float64(0.07536453209994007), 'eval_n_ngrams_match_1': 13.6, 'eval_n_ngrams_match_2': 5.1, 'eval_n_ngrams_match_3': 3.0, 'eval_num_true_words': 38.9, 'eval_num_pred_words': 22.1, 'eval_bleu_score': np.float64(9.730428576597138), 'eval_bleu_score_sem': np.float64(4.545637056216745), 'eval_rouge_score': np.float64(0.5140666611941678), 'eval_exact_match': np.float64(0.0), 'eval_exact_match_sem': np.float64(0.0), 'eval_emb_cos_sim': 0.8158857226371765, 'eval_emb_cos_sim_sem': np.float64(0.054309101183238985), 'eval_emb_top1_equal': 0.30000001192092896, 'eval_emb_top1_equal_sem': np.float64(0.15275251825939698), 'eval_runtime': 95.7838, 'eval_samples_per_second': 0.104, 'eval_steps_per_second': 0.01}
+++ Evaluation Metrics +++
eval_loss: 12.47735595703125
eval_model_preparation_time: 0.0196
eval_pred_num_tokens: 31.0
eval_true_num_tokens: 46.900001525878906
eval_token_set_precision: 0.4611165272628586
eval_token_set_recall: 0.6260221051090615
eval_token_set_f1: 0.513638792395682
eval_token_set_f1_sem: 0.07536453209994007
eval_n_ngrams_match_1: 13.6
eval_n_ngrams_match_2: 5.1
eval_n_ngrams_match_3: 3.0
eval_num_true_words: 38.9
eval_num_pred_words: 22.1
eval_bleu_score: 9.730428576597138
eval_bleu_score_sem: 4.545637056216745
eval_rouge_score: 0.5140666611941678
eval_exact_match: 0.0
eval_exact_match_sem: 0.0
eval_emb_cos_sim: 0.8158857226371765
eval_emb_cos_sim_sem: 0.054309101183238985
eval_emb_top1_equal: 0.30000001192092896
eval_emb_top1_equal_sem: 0.15275251825939698
eval_runtime: 95.7838
eval_samples_per_second: 0.104
eval_steps_per_second: 0.01
Time taken: 95.81479024887085
Current memory usage: 3.36MB; Peak: 9.57MB
I1222 01:06:16.985187 2686832 torch/_dynamo/utils.py:399] TorchDynamo compilation metrics:
I1222 01:06:16.985187 2686832 torch/_dynamo/utils.py:399] Function    Runtimes (s)
I1222 01:06:16.985187 2686832 torch/_dynamo/utils.py:399] ----------  --------------
